const _astro_dataLayerContent = [["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.3","content-config-digest","f1fdf2c79429c63c","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,43,44,70,71,97,98,124,125,151,152,181,182,199,200,226,227,253,254,277,278,309,310],"cache-rules-everything",{id:11,data:13,body:18,filePath:19,digest:20,rendered:21},{title:14,date:15,excerpt:16,author:17},"Cache Rules Everything: A Practical Guide to Prompt Caching for Enterprise AI",["Date","2026-03-13T00:00:00.000Z"],"Part two in our series on context engineering. Prompt caching is the mechanism that makes long-running AI agents economically viable — and breaking it is easier than you think.","JD Fortress AI","In January we wrote about [the context engineering problem at the heart of enterprise RAG](/blog/context-is-not-free) — how the quality and structure of what you put in front of a model determines whether it produces reliable output or noise. That piece was about what to put in context, and why selectivity matters. This one is about the mechanism that enforces the economics of that discipline.\n\nThariq Shafi, an engineer on Anthropic’s Claude Code team, opened his post on prompt caching with a riff on the old engineering adage: “Cache Rules Everything Around Me.” It’s a deliberately irreverent framing, but the underlying claim is serious. Long-running AI agents — the kind that maintain context across many turns, process large document sets, and handle complex multi-step tasks — are only economically viable because of prompt caching. Without it, every turn of a conversation would reprocess the entire history from scratch. At the token volumes a production agent handles, that cost makes most serious applications financially impossible to run.\n\nShafi’s disclosure was striking not because the technology is obscure, but because of what it reveals about how production AI actually works. Drops in cache hit rate are treated as production incidents at Anthropic, triggering severity alerts and engineering response. Not degraded model quality. Not hallucination rates. The metric that pages engineers at three in the morning is how often the prefix cache is reused.\n\nUnderstanding why — and how to avoid breaking it — is the practical prerequisite for building anything that runs at scale.\n\n## How prefix matching actually works\n\nPrompt caching is a prefix match. When a request arrives at the API, everything from the start of the request up to a designated breakpoint gets hashed and stored. If the next request carries an identical prefix up to that point, the cached computation is reused. The model doesn’t reprocess those tokens — it picks up from the cache and continues.\n\nThat sounds straightforward. The fragility is in what “identical” means. One character difference anywhere in the prefix — a changed timestamp, a tool definition that appeared in a different order, a parameter that was updated silently — and the entire cache from that point onwards is invalidated. Full reprocessing at standard token prices, for every token that follows the break.\n\nThe Claude Code team describe this as “surprisingly fragile,” and the incidents they cite from their own history illustrate why. A detailed timestamp embedded in the static system prompt. Tool definitions being shuffled in non-deterministic order. A parameter update in one tool definition. Each of these caused silent cache misses across the entire product before anyone diagnosed the cause.\n\nSilently is the key word. The system still produces output. The model still answers. The only visible signal is the cost and latency going up — and unless you’re monitoring cache hit rates specifically, those can look like ordinary variance for days.\n\nThe economics make this worth caring about. On Claude Sonnet, cached tokens cost $0.30 per million. Uncached tokens cost $3.00. A tenfold difference. Consider a 50-turn session with a 200,000-token context: without caching, each turn costs roughly $0.60 in input tokens alone; with caching, subsequent turns drop to around $0.06. Across fifty turns, that’s the difference between $30 and $3.60 — for a single session. Multiply across millions of users and the arithmetic determines whether the product can exist at its current price. This is why the Claude Code team monitors cache performance with the same intensity it watches uptime.\n\n## The four rules\n\nThariq’s post distils the Claude Code team’s hard-won practice into a small number of rules. They’re worth stating plainly.\n\n**Static content goes first, dynamic content goes last.** The optimal order for a production AI system is: static system prompt and tool definitions first, then any content that varies per-project, then session-level context, then the actual conversation. Everything early in the prefix is shared across as many requests as possible. Everything variable comes at the end, where its changes affect only the tokens that follow it. This structure is what allows different users, different sessions, and different turns to share cached prefixes at each level of granularity.\n\n**Update via messages, not system prompt edits.** Suppose the model needs to know the current time, or the user has just modified a file, or some external state has changed. The instinct is to update the system prompt. Don’t. Editing the system prompt breaks the cache at the point of the edit, invalidating everything downstream. The correct approach is to inject the update into the next user message — the Claude Code team uses a `<system-reminder>` tag in the user turn — so the update reaches the model without touching the cached prefix.\n\n**Never add or remove tools mid-session.** Tool definitions sit in the cached prefix. Adding or removing a tool mid-conversation changes the prefix, which invalidates the cache for the rest of the session. This feels counterintuitive — it seems efficient to only give the model the tools it currently needs — but the cost of the cache miss outweighs any benefit from a cleaner tool list. Keep the full tool set consistent across every request in a session.\n\n**Never switch models mid-session.** Prompt caches are model-specific. A hundred thousand tokens of cached context built up with Opus cannot be transferred to Haiku. Switching models mid-session means rebuilding the cache from scratch on the new model — which may well cost more than simply having the original model answer the question. If you genuinely need a different model for a subtask, the correct approach is a subagent: the primary model prepares a focused handoff message and the subtask runs as a separate session with its own cache.\n\n## Designing features around the constraint\n\nThe most instructive part of Shafi’s account isn’t the rules themselves — it’s the examples of features that were designed around the caching constraint rather than against it.\n\nPlan Mode restricts the model to read-only operations. The instinct is to implement this by swapping the tool set: when the user enters plan mode, strip out write tools; when they exit, restore them. Clean, conceptually simple. Also catastrophically expensive from a caching perspective, because every mode transition would invalidate the cached prefix and discard whatever context had built up. The actual implementation keeps every tool present in every request, always. Plan Mode is itself a tool — EnterPlanMode and ExitPlanMode are callable functions that inject a mode constraint via message, not by altering the tool definitions. The prefix stays identical across mode transitions. Cache survives intact. And because EnterPlanMode is a function the model can call itself, Claude Code sometimes enters plan mode autonomously when it detects a situation that warrants more careful handling — a useful emergent behaviour that exists only because the cache-safe design forced a more disciplined architecture.\n\nThe same logic governs tool loading. Claude Code can have dozens of MCP tools loaded at once, and including all their full schema definitions in every request would mean thousands of tokens of static definitions sitting between the system prompt and the first user message — expensive to build, and impossible to remove mid-session without breaking the cache. The solution is lightweight stubs: placeholder entries with just a tool name and a deferred-loading flag. The model can call a ToolSearch function to retrieve the full schema when it actually needs it. The stubs sit in the prefix, unchanging, in the same order, every request.\n\nCompaction — what happens when a conversation outgrows the context window — posed the same problem in a different form. A naive implementation summarises the conversation history using a separate API call with a different system prompt and no tools. Completely different prefix: full reprocessing cost for the entire conversation, every time. The cache-safe version uses the exact same system prompt, tool definitions, and conversation prefix as the parent request, appending the summarisation instruction as the final user message. From the API’s perspective it looks nearly identical to the previous request. Only the compaction prompt itself is new.\n\nEach of these — Plan Mode, tool stubs, compaction — arrived at a better design by asking the same question: how do we achieve this without touching the prefix?\n\n## What this means if you’re building private AI\n\nPeak Ji, who founded the Manus AI agent before its acquisition by Meta, arrived at the same conclusions independently. He described the KV-cache hit rate as “the single most important metric for a production-stage AI agent,” noting that Manus processes input and output tokens at roughly a 100:1 ratio — meaning 99% of every request’s cost sits in input processing. Ji rebuilt the Manus agent framework four times, each version discarded after hitting cost walls that made the product unviable. The third or fourth rebuild was the one that finally treated cache efficiency as an architectural constraint from the start.\n\nThe convergence is significant. Anthropic and Manus are different products, different user bases, different underlying models in some cases. They arrived at identical rules because the constraint is the same.\n\nFor enterprises running AI on their own hardware, the arithmetic is the same even without per-token pricing. Whether you’re paying $3.00 per million tokens to a cloud provider or burning GPU cycles on your own H100s, a cache miss means your hardware is reprocessing the same system prompt, the same document set, the same conversation history, on every single turn. On local inference, cache hits reduce time-to-first-token and free up GPU capacity for other requests. A pipeline that runs efficiently at 500 queries a day can collapse at 5,000 if the cache hit rate drops from 85% to 50% — not because the model got worse, but because the prefix stopped being stable.\n\nWhen we design [RAG pipelines](/blog/what-is-rag) and agent systems for clients, the four rules above are baked into the architecture before the first line of code is written. The ordering of context layers, the handling of state updates, the management of tools across a session — these are decisions that are very expensive to undo later. The lesson from both the Claude Code team and the Manus team is the same: design around the prefix from day one, or rebuild later.\n\nMonitor your cache hit rate like you monitor uptime. It is the single most informative signal about whether your AI system is running as designed. A few percentage points of cache miss rate, compounding across millions of requests, translates into infrastructure spend and user-facing latency in ways that are easy to miss until they become impossible to ignore.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.*","src/content/blog/cache-rules-everything.md","6d072dfde6b9a33b",{html:22,metadata:23},"<p>In January we wrote about <a href=\"/blog/context-is-not-free\">the context engineering problem at the heart of enterprise RAG</a> — how the quality and structure of what you put in front of a model determines whether it produces reliable output or noise. That piece was about what to put in context, and why selectivity matters. This one is about the mechanism that enforces the economics of that discipline.</p>\n<p>Thariq Shafi, an engineer on Anthropic’s Claude Code team, opened his post on prompt caching with a riff on the old engineering adage: “Cache Rules Everything Around Me.” It’s a deliberately irreverent framing, but the underlying claim is serious. Long-running AI agents — the kind that maintain context across many turns, process large document sets, and handle complex multi-step tasks — are only economically viable because of prompt caching. Without it, every turn of a conversation would reprocess the entire history from scratch. At the token volumes a production agent handles, that cost makes most serious applications financially impossible to run.</p>\n<p>Shafi’s disclosure was striking not because the technology is obscure, but because of what it reveals about how production AI actually works. Drops in cache hit rate are treated as production incidents at Anthropic, triggering severity alerts and engineering response. Not degraded model quality. Not hallucination rates. The metric that pages engineers at three in the morning is how often the prefix cache is reused.</p>\n<p>Understanding why — and how to avoid breaking it — is the practical prerequisite for building anything that runs at scale.</p>\n<h2 id=\"how-prefix-matching-actually-works\">How prefix matching actually works</h2>\n<p>Prompt caching is a prefix match. When a request arrives at the API, everything from the start of the request up to a designated breakpoint gets hashed and stored. If the next request carries an identical prefix up to that point, the cached computation is reused. The model doesn’t reprocess those tokens — it picks up from the cache and continues.</p>\n<p>That sounds straightforward. The fragility is in what “identical” means. One character difference anywhere in the prefix — a changed timestamp, a tool definition that appeared in a different order, a parameter that was updated silently — and the entire cache from that point onwards is invalidated. Full reprocessing at standard token prices, for every token that follows the break.</p>\n<p>The Claude Code team describe this as “surprisingly fragile,” and the incidents they cite from their own history illustrate why. A detailed timestamp embedded in the static system prompt. Tool definitions being shuffled in non-deterministic order. A parameter update in one tool definition. Each of these caused silent cache misses across the entire product before anyone diagnosed the cause.</p>\n<p>Silently is the key word. The system still produces output. The model still answers. The only visible signal is the cost and latency going up — and unless you’re monitoring cache hit rates specifically, those can look like ordinary variance for days.</p>\n<p>The economics make this worth caring about. On Claude Sonnet, cached tokens cost $0.30 per million. Uncached tokens cost $3.00. A tenfold difference. Consider a 50-turn session with a 200,000-token context: without caching, each turn costs roughly $0.60 in input tokens alone; with caching, subsequent turns drop to around $0.06. Across fifty turns, that’s the difference between $30 and $3.60 — for a single session. Multiply across millions of users and the arithmetic determines whether the product can exist at its current price. This is why the Claude Code team monitors cache performance with the same intensity it watches uptime.</p>\n<h2 id=\"the-four-rules\">The four rules</h2>\n<p>Thariq’s post distils the Claude Code team’s hard-won practice into a small number of rules. They’re worth stating plainly.</p>\n<p><strong>Static content goes first, dynamic content goes last.</strong> The optimal order for a production AI system is: static system prompt and tool definitions first, then any content that varies per-project, then session-level context, then the actual conversation. Everything early in the prefix is shared across as many requests as possible. Everything variable comes at the end, where its changes affect only the tokens that follow it. This structure is what allows different users, different sessions, and different turns to share cached prefixes at each level of granularity.</p>\n<p><strong>Update via messages, not system prompt edits.</strong> Suppose the model needs to know the current time, or the user has just modified a file, or some external state has changed. The instinct is to update the system prompt. Don’t. Editing the system prompt breaks the cache at the point of the edit, invalidating everything downstream. The correct approach is to inject the update into the next user message — the Claude Code team uses a <code>&#x3C;system-reminder></code> tag in the user turn — so the update reaches the model without touching the cached prefix.</p>\n<p><strong>Never add or remove tools mid-session.</strong> Tool definitions sit in the cached prefix. Adding or removing a tool mid-conversation changes the prefix, which invalidates the cache for the rest of the session. This feels counterintuitive — it seems efficient to only give the model the tools it currently needs — but the cost of the cache miss outweighs any benefit from a cleaner tool list. Keep the full tool set consistent across every request in a session.</p>\n<p><strong>Never switch models mid-session.</strong> Prompt caches are model-specific. A hundred thousand tokens of cached context built up with Opus cannot be transferred to Haiku. Switching models mid-session means rebuilding the cache from scratch on the new model — which may well cost more than simply having the original model answer the question. If you genuinely need a different model for a subtask, the correct approach is a subagent: the primary model prepares a focused handoff message and the subtask runs as a separate session with its own cache.</p>\n<h2 id=\"designing-features-around-the-constraint\">Designing features around the constraint</h2>\n<p>The most instructive part of Shafi’s account isn’t the rules themselves — it’s the examples of features that were designed around the caching constraint rather than against it.</p>\n<p>Plan Mode restricts the model to read-only operations. The instinct is to implement this by swapping the tool set: when the user enters plan mode, strip out write tools; when they exit, restore them. Clean, conceptually simple. Also catastrophically expensive from a caching perspective, because every mode transition would invalidate the cached prefix and discard whatever context had built up. The actual implementation keeps every tool present in every request, always. Plan Mode is itself a tool — EnterPlanMode and ExitPlanMode are callable functions that inject a mode constraint via message, not by altering the tool definitions. The prefix stays identical across mode transitions. Cache survives intact. And because EnterPlanMode is a function the model can call itself, Claude Code sometimes enters plan mode autonomously when it detects a situation that warrants more careful handling — a useful emergent behaviour that exists only because the cache-safe design forced a more disciplined architecture.</p>\n<p>The same logic governs tool loading. Claude Code can have dozens of MCP tools loaded at once, and including all their full schema definitions in every request would mean thousands of tokens of static definitions sitting between the system prompt and the first user message — expensive to build, and impossible to remove mid-session without breaking the cache. The solution is lightweight stubs: placeholder entries with just a tool name and a deferred-loading flag. The model can call a ToolSearch function to retrieve the full schema when it actually needs it. The stubs sit in the prefix, unchanging, in the same order, every request.</p>\n<p>Compaction — what happens when a conversation outgrows the context window — posed the same problem in a different form. A naive implementation summarises the conversation history using a separate API call with a different system prompt and no tools. Completely different prefix: full reprocessing cost for the entire conversation, every time. The cache-safe version uses the exact same system prompt, tool definitions, and conversation prefix as the parent request, appending the summarisation instruction as the final user message. From the API’s perspective it looks nearly identical to the previous request. Only the compaction prompt itself is new.</p>\n<p>Each of these — Plan Mode, tool stubs, compaction — arrived at a better design by asking the same question: how do we achieve this without touching the prefix?</p>\n<h2 id=\"what-this-means-if-youre-building-private-ai\">What this means if you’re building private AI</h2>\n<p>Peak Ji, who founded the Manus AI agent before its acquisition by Meta, arrived at the same conclusions independently. He described the KV-cache hit rate as “the single most important metric for a production-stage AI agent,” noting that Manus processes input and output tokens at roughly a 100:1 ratio — meaning 99% of every request’s cost sits in input processing. Ji rebuilt the Manus agent framework four times, each version discarded after hitting cost walls that made the product unviable. The third or fourth rebuild was the one that finally treated cache efficiency as an architectural constraint from the start.</p>\n<p>The convergence is significant. Anthropic and Manus are different products, different user bases, different underlying models in some cases. They arrived at identical rules because the constraint is the same.</p>\n<p>For enterprises running AI on their own hardware, the arithmetic is the same even without per-token pricing. Whether you’re paying $3.00 per million tokens to a cloud provider or burning GPU cycles on your own H100s, a cache miss means your hardware is reprocessing the same system prompt, the same document set, the same conversation history, on every single turn. On local inference, cache hits reduce time-to-first-token and free up GPU capacity for other requests. A pipeline that runs efficiently at 500 queries a day can collapse at 5,000 if the cache hit rate drops from 85% to 50% — not because the model got worse, but because the prefix stopped being stable.</p>\n<p>When we design <a href=\"/blog/what-is-rag\">RAG pipelines</a> and agent systems for clients, the four rules above are baked into the architecture before the first line of code is written. The ordering of context layers, the handling of state updates, the management of tools across a session — these are decisions that are very expensive to undo later. The lesson from both the Claude Code team and the Manus team is the same: design around the prefix from day one, or rebuild later.</p>\n<p>Monitor your cache hit rate like you monitor uptime. It is the single most informative signal about whether your AI system is running as designed. A few percentage points of cache miss rate, compounding across millions of requests, translates into infrastructure spend and user-facing latency in ways that are easy to miss until they become impossible to ignore.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.</em></p>",{headings:24,localImagePaths:38,remoteImagePaths:39,frontmatter:40,imagePaths:42},[25,29,32,35],{depth:26,slug:27,text:28},2,"how-prefix-matching-actually-works","How prefix matching actually works",{depth:26,slug:30,text:31},"the-four-rules","The four rules",{depth:26,slug:33,text:34},"designing-features-around-the-constraint","Designing features around the constraint",{depth:26,slug:36,text:37},"what-this-means-if-youre-building-private-ai","What this means if you’re building private AI",[],[],{title:14,date:41,excerpt:16,author:17},["Date","2026-03-13T00:00:00.000Z"],[],"chatgpt-discovery-legal-risk",{id:43,data:45,body:49,filePath:50,digest:51,rendered:52},{title:46,date:47,excerpt:48,author:17},"ChatGPT Won’t Forget: The Legal Discovery Risk Every UK Business Should Understand",["Date","2026-01-27T00:00:00.000Z"],"A US court has ordered OpenAI to hand over 20 million ChatGPT conversations. The case started as a copyright dispute. The implications reach every business that uses cloud AI for anything sensitive.","Last autumn, OpenAI found itself in an unusual position: a federal court ordered the company to preserve every ChatGPT conversation — including those its users had already deleted. The cause was the ongoing *New York Times v. OpenAI* copyright lawsuit, and the logic was straightforward enough. If OpenAI continued to delete user chats on its standard 30-day cycle, potentially crucial evidence could disappear before anyone had a chance to examine it. So Judge Ona Wang of the Southern District of New York stepped in.\n\nThat preservation order, issued in May 2025, ran until September. But the litigation continued to accelerate. By November, OpenAI had been directed to hand over approximately 20 million ChatGPT chat logs to the Times’s legal team. In December, a district judge affirmed that ruling in full, rejecting OpenAI’s privacy-based objections. Whatever reassurance users had drawn from pressing “delete” was, in the space of a few months, thoroughly dismantled.\n\nFor UK businesses, it’s easy to watch this as a distant American dispute. It isn’t.\n\n## “Delete” has never meant what most people assume\n\nThe mechanics are worth understanding, because they’re not intuitive. When a standard ChatGPT user deletes a conversation, the chat disappears from their account interface. It does not disappear from OpenAI’s servers. Under the company’s normal policy, it would be purged after 30 days. Under a preservation order — or in any circumstance where OpenAI decides retention is legally necessary — it stays indefinitely, held in a segregated system accessible only to a small internal legal and security team.\n\nOpenAI CEO Sam Altman has argued publicly that AI conversations should carry something like the confidentiality afforded to consultations with a doctor or a lawyer. “We believe people should have AI privilege,” he said. That position reflects the company’s genuine discomfort with the situation. It also reflects how far AI law has yet to travel. Courts, at least for now, are applying existing discovery principles to AI chat logs — and those principles don’t recognise any such privilege.\n\nThe practical upshot: for users on standard Free, Plus, Pro, or Team plans, and for most API clients without special contractual protections, the delete button is better understood as a visibility toggle than an erasure mechanism.\n\n## Two distinct risks for UK businesses\n\nThe first risk is straightforward: your organisation’s ChatGPT conversations could become evidence in US legal proceedings, even if your business has no direct connection to the case. The [CLOUD Act](/blog/why-law-firms-cant-afford-cloud-ai) means US federal authorities can compel US-headquartered AI providers — OpenAI, Microsoft, Google — to produce data regardless of where the user is located or where the data is nominally stored. You don’t need to be a party to a US lawsuit for your chats to become relevant.\n\nThe second risk is closer to home. UK litigation carries its own disclosure obligations. If a dispute arises — commercial, employment, regulatory — and AI conversations are relevant to the facts in question, a UK court can order disclosure of any documents in your control, including communications with third-party platforms. “We deleted it” is not a defence if the data still exists on a vendor’s servers and is recoverable. The question of whether AI chat logs constitute “documents” for the purposes of UK disclosure is still being tested, but the direction of travel is clear.\n\nHelen, a compliance director at a mid-sized financial services firm in Manchester, put it to us plainly: “We had staff using ChatGPT to draft client-facing materials and internal analysis. No one had thought about what happens to those conversations if we end up in a regulatory investigation. We assumed the data was transient. It isn’t.”\n\n## The structural fix already exists\n\nThis is where the *New York Times* case is inadvertently clarifying. The preservation order, and the subsequent 20 million chat log ruling, applied to OpenAI’s consumer and standard commercial users — not to Enterprise customers with Zero Data Retention contracts, and not to users operating entirely on private or on-premises infrastructure.\n\nEnterprise-tier agreements with OpenAI exclude user inputs from model training and offer stronger deletion guarantees. But they still involve data transiting OpenAI’s servers, and they still carry the underlying exposure to US legal process. The more durable solution is one that doesn’t rely on contractual protections at all: AI that runs on [infrastructure you control](/blog/what-is-rag), where the conversation logs are yours alone and no third-party vendor can be compelled to produce them, because they never had them.\n\nThat isn’t a futuristic arrangement. It’s a configuration choice that organisations across regulated sectors are making now — for exactly the reasons this case has put in sharp relief.\n\nIf your business uses AI tools for anything sensitive — client work, commercial negotiations, internal analysis, strategy — it’s worth asking a simple question: if a court asked for everything you’ve ever typed into that tool, what would they find, and who has it?\n\nWe’re happy to talk through what genuinely private AI looks like for your organisation, and how quickly it can be operational.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.*","src/content/blog/chatgpt-discovery-legal-risk.md","cda4d5421db6338c",{html:53,metadata:54},"<p>Last autumn, OpenAI found itself in an unusual position: a federal court ordered the company to preserve every ChatGPT conversation — including those its users had already deleted. The cause was the ongoing <em>New York Times v. OpenAI</em> copyright lawsuit, and the logic was straightforward enough. If OpenAI continued to delete user chats on its standard 30-day cycle, potentially crucial evidence could disappear before anyone had a chance to examine it. So Judge Ona Wang of the Southern District of New York stepped in.</p>\n<p>That preservation order, issued in May 2025, ran until September. But the litigation continued to accelerate. By November, OpenAI had been directed to hand over approximately 20 million ChatGPT chat logs to the Times’s legal team. In December, a district judge affirmed that ruling in full, rejecting OpenAI’s privacy-based objections. Whatever reassurance users had drawn from pressing “delete” was, in the space of a few months, thoroughly dismantled.</p>\n<p>For UK businesses, it’s easy to watch this as a distant American dispute. It isn’t.</p>\n<h2 id=\"delete-has-never-meant-what-most-people-assume\">“Delete” has never meant what most people assume</h2>\n<p>The mechanics are worth understanding, because they’re not intuitive. When a standard ChatGPT user deletes a conversation, the chat disappears from their account interface. It does not disappear from OpenAI’s servers. Under the company’s normal policy, it would be purged after 30 days. Under a preservation order — or in any circumstance where OpenAI decides retention is legally necessary — it stays indefinitely, held in a segregated system accessible only to a small internal legal and security team.</p>\n<p>OpenAI CEO Sam Altman has argued publicly that AI conversations should carry something like the confidentiality afforded to consultations with a doctor or a lawyer. “We believe people should have AI privilege,” he said. That position reflects the company’s genuine discomfort with the situation. It also reflects how far AI law has yet to travel. Courts, at least for now, are applying existing discovery principles to AI chat logs — and those principles don’t recognise any such privilege.</p>\n<p>The practical upshot: for users on standard Free, Plus, Pro, or Team plans, and for most API clients without special contractual protections, the delete button is better understood as a visibility toggle than an erasure mechanism.</p>\n<h2 id=\"two-distinct-risks-for-uk-businesses\">Two distinct risks for UK businesses</h2>\n<p>The first risk is straightforward: your organisation’s ChatGPT conversations could become evidence in US legal proceedings, even if your business has no direct connection to the case. The <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">CLOUD Act</a> means US federal authorities can compel US-headquartered AI providers — OpenAI, Microsoft, Google — to produce data regardless of where the user is located or where the data is nominally stored. You don’t need to be a party to a US lawsuit for your chats to become relevant.</p>\n<p>The second risk is closer to home. UK litigation carries its own disclosure obligations. If a dispute arises — commercial, employment, regulatory — and AI conversations are relevant to the facts in question, a UK court can order disclosure of any documents in your control, including communications with third-party platforms. “We deleted it” is not a defence if the data still exists on a vendor’s servers and is recoverable. The question of whether AI chat logs constitute “documents” for the purposes of UK disclosure is still being tested, but the direction of travel is clear.</p>\n<p>Helen, a compliance director at a mid-sized financial services firm in Manchester, put it to us plainly: “We had staff using ChatGPT to draft client-facing materials and internal analysis. No one had thought about what happens to those conversations if we end up in a regulatory investigation. We assumed the data was transient. It isn’t.”</p>\n<h2 id=\"the-structural-fix-already-exists\">The structural fix already exists</h2>\n<p>This is where the <em>New York Times</em> case is inadvertently clarifying. The preservation order, and the subsequent 20 million chat log ruling, applied to OpenAI’s consumer and standard commercial users — not to Enterprise customers with Zero Data Retention contracts, and not to users operating entirely on private or on-premises infrastructure.</p>\n<p>Enterprise-tier agreements with OpenAI exclude user inputs from model training and offer stronger deletion guarantees. But they still involve data transiting OpenAI’s servers, and they still carry the underlying exposure to US legal process. The more durable solution is one that doesn’t rely on contractual protections at all: AI that runs on <a href=\"/blog/what-is-rag\">infrastructure you control</a>, where the conversation logs are yours alone and no third-party vendor can be compelled to produce them, because they never had them.</p>\n<p>That isn’t a futuristic arrangement. It’s a configuration choice that organisations across regulated sectors are making now — for exactly the reasons this case has put in sharp relief.</p>\n<p>If your business uses AI tools for anything sensitive — client work, commercial negotiations, internal analysis, strategy — it’s worth asking a simple question: if a court asked for everything you’ve ever typed into that tool, what would they find, and who has it?</p>\n<p>We’re happy to talk through what genuinely private AI looks like for your organisation, and how quickly it can be operational.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.</em></p>",{headings:55,localImagePaths:65,remoteImagePaths:66,frontmatter:67,imagePaths:69},[56,59,62],{depth:26,slug:57,text:58},"delete-has-never-meant-what-most-people-assume","“Delete” has never meant what most people assume",{depth:26,slug:60,text:61},"two-distinct-risks-for-uk-businesses","Two distinct risks for UK businesses",{depth:26,slug:63,text:64},"the-structural-fix-already-exists","The structural fix already exists",[],[],{title:46,date:68,excerpt:48,author:17},["Date","2026-01-27T00:00:00.000Z"],[],"eniac-moment-future-on-premise-ai",{id:70,data:72,body:76,filePath:77,digest:78,rendered:79},{title:73,date:74,excerpt:75,author:17},"The ENIAC Moment: Why We’re Building for the AI That’s Coming",["Date","2026-02-27T00:00:00.000Z"],"A startup just built a chip that runs an AI model at 17,000 tokens per second, using a tenth of the power of a GPU. It’s a glimpse at a future that changes everything about how private AI gets deployed — and we think we’re building the right things to meet it.","In 1945, ENIAC filled a room the size of a house. It weighed 30 tonnes, consumed 150 kilowatts of power, and required a team of engineers just to keep it operational. It was also, for its moment, genuinely transformative — a demonstration that machines could do things previously thought to require human minds.\n\nNobody looked at ENIAC and predicted the smartphone. The transistor wasn’t obvious from vacuum tubes. And yet the trajectory from one to the other is, in hindsight, almost inevitable: every technology that matters eventually finds its most efficient form. Computing didn’t stay room-sized. It became a chip in your pocket, drawing milliwatts, costing pennies.\n\nA startup called Taalas published something this week that we’ve been thinking about since.\n\nThey’ve built custom silicon — not a general-purpose GPU, but a chip designed from the ground up around a specific AI model. Their first product hard-wires Llama 3.1 8B directly into the silicon, eliminating the divide between memory and compute that underpins most of the complexity and cost in modern AI infrastructure. The results are striking: 17,000 tokens per second per user. That’s roughly ten times faster than anything currently available at scale. It consumes a tenth of the power of a comparable GPU system and costs around twenty times less to build.\n\nIt runs without liquid cooling, high-bandwidth memory stacks, advanced packaging, or any of the exotic engineering that makes modern AI hardware both impressive and impractical for most deployment contexts.\n\nENIAC, meet the transistor.\n\n## This isn’t a product review — it’s a direction signal\n\nWe want to be clear about what Taalas has actually built and what it hasn’t. Their first chip runs an 8B model — capable and useful, but not on the frontier. The hard-wired architecture trades some flexibility for extraordinary efficiency. They’ve acknowledged quality trade-offs from aggressive quantisation in their current generation. This is genuinely early-stage technology, and it’s right to hold it alongside appropriate caveats.\n\nBut the direction it points is not speculative at all.\n\nThe history of computation is a history of specialisation. General-purpose hardware solves general problems adequately. Purpose-built hardware solves specific problems extraordinarily well. When the specific problem is important enough — and AI inference is arguably the most important computational workload humanity has ever faced — specialisation finds its moment.\n\nTaalas is not alone. The broader industry is moving the same direction. Groq has been chasing inference efficiency with its Language Processing Units. Cerebras builds wafer-scale chips that eliminate off-chip memory bottlenecks. Apple’s Neural Engine, built into every M-series chip, is an early-stage example of the same principle embedded in a consumer device. The trajectory is clear: inference will get faster, cheaper, and smaller.\n\n## What this means for the way we think about our product\n\nWe deploy AI privately, on-premises, for regulated UK businesses. Today, that means a piece of hardware — a Mac Studio, or a custom server — sitting quietly in a client’s office or server room, running frontier models locally. It means no data leaving the building, no vendor with a legal obligation to [preserve or disclose conversation logs](/blog/chatgpt-discovery-legal-risk), no dependency on a cloud provider’s pricing decisions or data governance policies.\n\nThe hardware is capable. But it’s also bigger, more expensive, and more power-hungry than it needs to be — because the underlying technology hasn’t yet found its most efficient form.\n\nHere’s the question we ask ourselves: what does this look like in five years, if the trajectory Taalas represents continues?\n\nWhat if a law firm’s entire private AI infrastructure — a model running at thousands of tokens per second, trained on their documents, integrated with their practice management system — fit in something the size of a broadband router? What if it drew less power than a desk lamp? What if the hardware cost a few hundred pounds rather than several thousand?\n\nWe don’t know exactly how that future arrives. But we believe it arrives. And the firms building the software layer, the client relationships, the data pipelines, and the deployment expertise now are the ones positioned to benefit most when it does.\n\n## The right things, in the right order\n\nThere’s a version of this moment where we wait — where we say the hardware isn’t mature enough, the models aren’t good enough, the economics aren’t there yet. That version exists, and it’s a reasonable position.\n\nWe’ve taken a different one. The clients we’re working with today — law firms and [in-house legal teams](/blog/gen-ai-for-in-house-lawyers) handling genuinely sensitive work — aren’t waiting for perfect. They want capable, private, and trustworthy now. The hardware available today is good enough to deliver that. What we’re building on top of it — the [RAG pipelines](/blog/what-is-rag) over client document libraries, the integrations, the compliance documentation, the institutional knowledge of how these systems behave in legal practice contexts — that compounds over time regardless of what the underlying silicon looks like.\n\nWhen the Taalas-shaped future arrives, or something like it, we won’t need to rebuild from scratch. We’ll swap in the better hardware, and everything we’ve built on top of it gets dramatically more capable overnight.\n\nThat’s the bet. It feels like a good one.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises AI for regulated UK businesses. We’re building for today and for what’s coming — get in touch if you’d like to talk about what that could look like for your organisation.*","src/content/blog/eniac-moment-future-on-premise-ai.md","4f2ff7446a1dadce",{html:80,metadata:81},"<p>In 1945, ENIAC filled a room the size of a house. It weighed 30 tonnes, consumed 150 kilowatts of power, and required a team of engineers just to keep it operational. It was also, for its moment, genuinely transformative — a demonstration that machines could do things previously thought to require human minds.</p>\n<p>Nobody looked at ENIAC and predicted the smartphone. The transistor wasn’t obvious from vacuum tubes. And yet the trajectory from one to the other is, in hindsight, almost inevitable: every technology that matters eventually finds its most efficient form. Computing didn’t stay room-sized. It became a chip in your pocket, drawing milliwatts, costing pennies.</p>\n<p>A startup called Taalas published something this week that we’ve been thinking about since.</p>\n<p>They’ve built custom silicon — not a general-purpose GPU, but a chip designed from the ground up around a specific AI model. Their first product hard-wires Llama 3.1 8B directly into the silicon, eliminating the divide between memory and compute that underpins most of the complexity and cost in modern AI infrastructure. The results are striking: 17,000 tokens per second per user. That’s roughly ten times faster than anything currently available at scale. It consumes a tenth of the power of a comparable GPU system and costs around twenty times less to build.</p>\n<p>It runs without liquid cooling, high-bandwidth memory stacks, advanced packaging, or any of the exotic engineering that makes modern AI hardware both impressive and impractical for most deployment contexts.</p>\n<p>ENIAC, meet the transistor.</p>\n<h2 id=\"this-isnt-a-product-review--its-a-direction-signal\">This isn’t a product review — it’s a direction signal</h2>\n<p>We want to be clear about what Taalas has actually built and what it hasn’t. Their first chip runs an 8B model — capable and useful, but not on the frontier. The hard-wired architecture trades some flexibility for extraordinary efficiency. They’ve acknowledged quality trade-offs from aggressive quantisation in their current generation. This is genuinely early-stage technology, and it’s right to hold it alongside appropriate caveats.</p>\n<p>But the direction it points is not speculative at all.</p>\n<p>The history of computation is a history of specialisation. General-purpose hardware solves general problems adequately. Purpose-built hardware solves specific problems extraordinarily well. When the specific problem is important enough — and AI inference is arguably the most important computational workload humanity has ever faced — specialisation finds its moment.</p>\n<p>Taalas is not alone. The broader industry is moving the same direction. Groq has been chasing inference efficiency with its Language Processing Units. Cerebras builds wafer-scale chips that eliminate off-chip memory bottlenecks. Apple’s Neural Engine, built into every M-series chip, is an early-stage example of the same principle embedded in a consumer device. The trajectory is clear: inference will get faster, cheaper, and smaller.</p>\n<h2 id=\"what-this-means-for-the-way-we-think-about-our-product\">What this means for the way we think about our product</h2>\n<p>We deploy AI privately, on-premises, for regulated UK businesses. Today, that means a piece of hardware — a Mac Studio, or a custom server — sitting quietly in a client’s office or server room, running frontier models locally. It means no data leaving the building, no vendor with a legal obligation to <a href=\"/blog/chatgpt-discovery-legal-risk\">preserve or disclose conversation logs</a>, no dependency on a cloud provider’s pricing decisions or data governance policies.</p>\n<p>The hardware is capable. But it’s also bigger, more expensive, and more power-hungry than it needs to be — because the underlying technology hasn’t yet found its most efficient form.</p>\n<p>Here’s the question we ask ourselves: what does this look like in five years, if the trajectory Taalas represents continues?</p>\n<p>What if a law firm’s entire private AI infrastructure — a model running at thousands of tokens per second, trained on their documents, integrated with their practice management system — fit in something the size of a broadband router? What if it drew less power than a desk lamp? What if the hardware cost a few hundred pounds rather than several thousand?</p>\n<p>We don’t know exactly how that future arrives. But we believe it arrives. And the firms building the software layer, the client relationships, the data pipelines, and the deployment expertise now are the ones positioned to benefit most when it does.</p>\n<h2 id=\"the-right-things-in-the-right-order\">The right things, in the right order</h2>\n<p>There’s a version of this moment where we wait — where we say the hardware isn’t mature enough, the models aren’t good enough, the economics aren’t there yet. That version exists, and it’s a reasonable position.</p>\n<p>We’ve taken a different one. The clients we’re working with today — law firms and <a href=\"/blog/gen-ai-for-in-house-lawyers\">in-house legal teams</a> handling genuinely sensitive work — aren’t waiting for perfect. They want capable, private, and trustworthy now. The hardware available today is good enough to deliver that. What we’re building on top of it — the <a href=\"/blog/what-is-rag\">RAG pipelines</a> over client document libraries, the integrations, the compliance documentation, the institutional knowledge of how these systems behave in legal practice contexts — that compounds over time regardless of what the underlying silicon looks like.</p>\n<p>When the Taalas-shaped future arrives, or something like it, we won’t need to rebuild from scratch. We’ll swap in the better hardware, and everything we’ve built on top of it gets dramatically more capable overnight.</p>\n<p>That’s the bet. It feels like a good one.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises AI for regulated UK businesses. We’re building for today and for what’s coming — get in touch if you’d like to talk about what that could look like for your organisation.</em></p>",{headings:82,localImagePaths:92,remoteImagePaths:93,frontmatter:94,imagePaths:96},[83,86,89],{depth:26,slug:84,text:85},"this-isnt-a-product-review--its-a-direction-signal","This isn’t a product review — it’s a direction signal",{depth:26,slug:87,text:88},"what-this-means-for-the-way-we-think-about-our-product","What this means for the way we think about our product",{depth:26,slug:90,text:91},"the-right-things-in-the-right-order","The right things, in the right order",[],[],{title:73,date:95,excerpt:75,author:17},["Date","2026-02-27T00:00:00.000Z"],[],"context-is-not-free",{id:97,data:99,body:103,filePath:104,digest:105,rendered:106},{title:100,date:101,excerpt:102,author:17},"Context Is Not Free: The Engineering Problem at the Heart of Enterprise RAG",["Date","2026-01-13T00:00:00.000Z"],"Every time context windows grow larger, someone declares RAG obsolete. They're wrong - and the research explains exactly why dumping everything into a model's context is a costly mistake.","Every few months, a new model arrives with a dramatically larger context window, and the same debate restarts: is RAG dead? When Llama 4 Scout launched with a ten-million-token context window, the argument went roughly like this - if you can fit your entire document library into a single prompt, why bother with retrieval at all? Just throw everything in and let the model sort it out.\n\nIt's a tempting position. It's also wrong, and the reason matters for anyone building AI systems that need to be reliable.\n\nThe core problem isn't storage - it's signal quality. A model's context window is not a hard drive. Every token in the context influences every token in the output. The larger the context, the more opportunity for noise, contradiction, and irrelevance to shape the response. Drew Breunig, writing on his research blog, put it plainly: treat your context like a junk drawer, and the junk will influence your response. This isn't opinion - it's a pattern the research community has been documenting with increasing rigour.\n\n## Four ways context fails\n\nBreunig's taxonomy of context failures is worth knowing, because each failure mode shows up in production systems in ways that are often misattributed to the underlying model.\n\n**Context poisoning** is the most insidious. When a hallucination or factual error enters the context - from a badly-retrieved document, a malformed data source, or a prior model output - it gets referenced and reinforced in every subsequent response. The model doesn't flag the error; it builds on it. In a legal or financial context, that compounding effect can carry a system from a minor retrieval mistake to a confidently-stated wrong conclusion.\n\n**Context distraction** is well-documented: as the context grows, models over-index on what's in front of them and rely less on what they learned during training. A team building a Gemini 2.5 Pro agent for a game environment observed the effect empirically - beyond roughly 100,000 tokens, the agent stopped synthesising novel plans and started repeating actions from its own history. Gemini 2.5 Pro supports over one million tokens. The failure began at a tenth of its theoretical limit.\n\n**Context confusion** is what happens when the context contains accurate but irrelevant information - the model finds correlations that aren't useful and generates responses that are technically grounded but practically wrong. And **context clash** occurs when accumulated information contradicts itself: two policy documents that were updated at different times, or tool definitions that overlap in ways the model can't resolve cleanly.\n\nThese aren't edge cases. They're the normal failure modes of systems that haven't been engineered to manage context deliberately.\n\n## The engineering response\n\nThere is a developed toolkit for handling these problems - and the research numbers behind each approach are worth understanding if you're evaluating AI systems for serious work.\n\nSelective retrieval ([RAG in its proper sense](/blog/what-is-rag)) remains foundational. The argument that bigger context windows make retrieval obsolete misunderstands what retrieval is for: it's not about fitting more in, it's about keeping irrelevant material out. A paper examining tool selection - a closely analogous problem - found that above thirty tool definitions in a prompt, performance begins degrading due to overlap and confusion. Above one hundred, failure was near-guaranteed. Applying retrieval techniques to select only the relevant tools improved accuracy by as much as 3× and yielded speed gains of 77%.\n\nMulti-agent architectures handle a related problem by isolating work into separate context threads. Rather than one agent accumulating an ever-growing context as it works through a complex task, subtasks are spun out to specialised agents, each working with a clean, focused context. Anthropic's internal evaluation of this approach found that a multi-agent research system outperformed a single-agent setup running the same underlying model by 90.2% on their benchmark. The gain isn't from more compute - it's from better information hygiene.\n\nContext pruning addresses bloat directly. Tools like Provence, a 1.75 GB open-source pruner, can assess a document against a specific query and strip out irrelevant content before it reaches the model. In one example, Provence reduced a long reference document by 95% while retaining the passages that actually answered the question. For a system processing thousands of queries against large knowledge bases, the compound effect on quality - and on inference cost - is significant.\n\n## What this means if you're running AI in a regulated environment\n\nFor enterprises in [legal, financial, or healthcare settings](/blog/why-law-firms-cant-afford-cloud-ai), context quality is not just a performance issue — it's a reliability and accountability issue. A RAG system that produces a confident, hallucinated legal summary because a stale document crept into its context isn't just inaccurate; it's [a liability](/blog/ai-third-party-privilege-ruling). The model doesn't know it's wrong. Neither will the user who reads the output without the domain expertise to question it.\n\nThis is why the architecture of a RAG pipeline matters far more than the size of the context window it uses. The choice of retrieval strategy, the handling of conflicting documents, the pruning of outdated content, the isolation of task-specific contexts from one another - these are engineering decisions that determine whether the system is trustworthy in practice, not just impressive in a demo.\n\nWhen we build RAG pipelines for clients, [context management sits at the centre of every design decision](/blog/cache-rules-everything). Not because it's technically interesting - though it is - but because the organisations we work with need to be able to rely on the output. A system that sometimes produces excellent answers and occasionally compounds a retrieval error into something worse is not a system that can be trusted with client work, regulatory submissions, or anything where the cost of a confident wrong answer is high.\n\nThe context window is a tool. How you fill it is the work.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you're exploring always-on, private AI teammates, get in touch for a confidential discussion - no pitch, just practical talk.*","src/content/blog/context-is-not-free.md","d9e5ba597cd28cc2",{html:107,metadata:108},"<p>Every few months, a new model arrives with a dramatically larger context window, and the same debate restarts: is RAG dead? When Llama 4 Scout launched with a ten-million-token context window, the argument went roughly like this - if you can fit your entire document library into a single prompt, why bother with retrieval at all? Just throw everything in and let the model sort it out.</p>\n<p>It’s a tempting position. It’s also wrong, and the reason matters for anyone building AI systems that need to be reliable.</p>\n<p>The core problem isn’t storage - it’s signal quality. A model’s context window is not a hard drive. Every token in the context influences every token in the output. The larger the context, the more opportunity for noise, contradiction, and irrelevance to shape the response. Drew Breunig, writing on his research blog, put it plainly: treat your context like a junk drawer, and the junk will influence your response. This isn’t opinion - it’s a pattern the research community has been documenting with increasing rigour.</p>\n<h2 id=\"four-ways-context-fails\">Four ways context fails</h2>\n<p>Breunig’s taxonomy of context failures is worth knowing, because each failure mode shows up in production systems in ways that are often misattributed to the underlying model.</p>\n<p><strong>Context poisoning</strong> is the most insidious. When a hallucination or factual error enters the context - from a badly-retrieved document, a malformed data source, or a prior model output - it gets referenced and reinforced in every subsequent response. The model doesn’t flag the error; it builds on it. In a legal or financial context, that compounding effect can carry a system from a minor retrieval mistake to a confidently-stated wrong conclusion.</p>\n<p><strong>Context distraction</strong> is well-documented: as the context grows, models over-index on what’s in front of them and rely less on what they learned during training. A team building a Gemini 2.5 Pro agent for a game environment observed the effect empirically - beyond roughly 100,000 tokens, the agent stopped synthesising novel plans and started repeating actions from its own history. Gemini 2.5 Pro supports over one million tokens. The failure began at a tenth of its theoretical limit.</p>\n<p><strong>Context confusion</strong> is what happens when the context contains accurate but irrelevant information - the model finds correlations that aren’t useful and generates responses that are technically grounded but practically wrong. And <strong>context clash</strong> occurs when accumulated information contradicts itself: two policy documents that were updated at different times, or tool definitions that overlap in ways the model can’t resolve cleanly.</p>\n<p>These aren’t edge cases. They’re the normal failure modes of systems that haven’t been engineered to manage context deliberately.</p>\n<h2 id=\"the-engineering-response\">The engineering response</h2>\n<p>There is a developed toolkit for handling these problems - and the research numbers behind each approach are worth understanding if you’re evaluating AI systems for serious work.</p>\n<p>Selective retrieval (<a href=\"/blog/what-is-rag\">RAG in its proper sense</a>) remains foundational. The argument that bigger context windows make retrieval obsolete misunderstands what retrieval is for: it’s not about fitting more in, it’s about keeping irrelevant material out. A paper examining tool selection - a closely analogous problem - found that above thirty tool definitions in a prompt, performance begins degrading due to overlap and confusion. Above one hundred, failure was near-guaranteed. Applying retrieval techniques to select only the relevant tools improved accuracy by as much as 3× and yielded speed gains of 77%.</p>\n<p>Multi-agent architectures handle a related problem by isolating work into separate context threads. Rather than one agent accumulating an ever-growing context as it works through a complex task, subtasks are spun out to specialised agents, each working with a clean, focused context. Anthropic’s internal evaluation of this approach found that a multi-agent research system outperformed a single-agent setup running the same underlying model by 90.2% on their benchmark. The gain isn’t from more compute - it’s from better information hygiene.</p>\n<p>Context pruning addresses bloat directly. Tools like Provence, a 1.75 GB open-source pruner, can assess a document against a specific query and strip out irrelevant content before it reaches the model. In one example, Provence reduced a long reference document by 95% while retaining the passages that actually answered the question. For a system processing thousands of queries against large knowledge bases, the compound effect on quality - and on inference cost - is significant.</p>\n<h2 id=\"what-this-means-if-youre-running-ai-in-a-regulated-environment\">What this means if you’re running AI in a regulated environment</h2>\n<p>For enterprises in <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">legal, financial, or healthcare settings</a>, context quality is not just a performance issue — it’s a reliability and accountability issue. A RAG system that produces a confident, hallucinated legal summary because a stale document crept into its context isn’t just inaccurate; it’s <a href=\"/blog/ai-third-party-privilege-ruling\">a liability</a>. The model doesn’t know it’s wrong. Neither will the user who reads the output without the domain expertise to question it.</p>\n<p>This is why the architecture of a RAG pipeline matters far more than the size of the context window it uses. The choice of retrieval strategy, the handling of conflicting documents, the pruning of outdated content, the isolation of task-specific contexts from one another - these are engineering decisions that determine whether the system is trustworthy in practice, not just impressive in a demo.</p>\n<p>When we build RAG pipelines for clients, <a href=\"/blog/cache-rules-everything\">context management sits at the centre of every design decision</a>. Not because it’s technically interesting - though it is - but because the organisations we work with need to be able to rely on the output. A system that sometimes produces excellent answers and occasionally compounds a retrieval error into something worse is not a system that can be trusted with client work, regulatory submissions, or anything where the cost of a confident wrong answer is high.</p>\n<p>The context window is a tool. How you fill it is the work.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion - no pitch, just practical talk.</em></p>",{headings:109,localImagePaths:119,remoteImagePaths:120,frontmatter:121,imagePaths:123},[110,113,116],{depth:26,slug:111,text:112},"four-ways-context-fails","Four ways context fails",{depth:26,slug:114,text:115},"the-engineering-response","The engineering response",{depth:26,slug:117,text:118},"what-this-means-if-youre-running-ai-in-a-regulated-environment","What this means if you’re running AI in a regulated environment",[],[],{title:100,date:122,excerpt:102,author:17},["Date","2026-01-13T00:00:00.000Z"],[],"gen-ai-for-in-house-lawyers",{id:124,data:126,body:130,filePath:131,digest:132,rendered:133},{title:127,date:128,excerpt:129,author:17},"From Stretched to Superhuman: What Generative AI Actually Does for the In-House Lawyer",["Date","2026-01-20T00:00:00.000Z"],"Most in-house legal teams are one or two people carrying the workload of ten. Generative AI doesn’t replace the lawyer’s judgment — it replaces the hours of work that came before the judgment started.","James is head of legal at a 300-person logistics company in Birmingham. He has one qualified solicitor working under him, a part-time paralegal, and somewhere in the region of four hundred live commercial relationships to manage at any one time. His Monday mornings start with a stack of contract redlines from procurement, a question from HR about a disciplinary procedure, and an email from the CFO asking whether a clause in their main haulage agreement actually means what the finance team thinks it means.\n\nHe isn’t unusual. The in-house legal function at most mid-sized UK businesses looks something like this: a small team, significant exposure, constant pressure to clear the queue. What varies is how long each task takes — and that’s precisely where generative AI is beginning to make a meaningful difference.\n\n## First-pass work is the biggest drain on in-house time\n\nThe most time-consuming part of in-house legal work often isn’t the hard thinking. It’s the scaffolding that has to happen before the hard thinking can begin. Reading a 45-page supplier agreement to flag the clauses that matter. Producing a first draft of an NDA from scratch. Pulling together the research memo that lets you give an opinion on a regulatory question you haven’t encountered before. Translating a board resolution into the right board minutes format.\n\nNone of this requires a qualified solicitor in the way that a nuanced commercial negotiation does. But it consumes qualified-solicitor hours at a significant rate.\n\nA well-deployed generative AI system can perform a first-pass review of a standard commercial contract in minutes, flagging non-standard clauses, identifying missing provisions, and summarising the document’s key obligations. The lawyer still makes every call — but they’re making those calls from an annotated starting position rather than a blank read-through. The research memo drafts itself from the AI’s synthesis of the relevant material; the lawyer checks it, adjusts it, and applies their judgment to the specific facts. The NDA starts at ninety percent rather than zero.\n\nAccording to the Association of Corporate Counsel’s latest survey, 91% of legal professionals who have adopted generative AI cite efficiency as its primary benefit. That figure isn’t surprising — but what’s notable is how quickly adoption has accelerated. In 2024, 23% of in-house counsel reported active use of GenAI tools. By 2025, that figure had more than doubled to 52%. Something shifted: the tools became good enough that the professional case for using them became difficult to ignore.\n\n## The research question that used to cost you an external brief\n\nThere’s a particular kind of question that in-house teams regularly face: specific enough to need real legal analysis, general enough that it shouldn’t require a three-day brief from external counsel at £350 per hour.\n\nEmployment law queries are a good example. So are data protection questions, supply chain contract disputes, and questions about implied terms in service agreements. These are areas where a competent in-house lawyer can give sound advice — but producing that advice historically required either significant research time or a call to an expensive specialist.\n\nGenerative AI changes the economics of this considerably. A system trained on the relevant legal material — UK employment law, GDPR guidance, commercial contract principles — can surface the applicable framework, the key cases, and the relevant regulatory positions in the time it takes to write the question. The lawyer still provides the legal advice; the AI provides the reference architecture that would previously have taken an afternoon to assemble.\n\nThe ACC survey found that 64% of in-house counsel expect generative AI to reduce their reliance on [external law firms](/blog/why-law-firms-cant-afford-cloud-ai). That’s not wishful thinking — it reflects a real shift in what an in-house team of two or three people can credibly handle without outsourcing.\n\n## The private advantage — why “which AI” matters more than “whether AI”\n\nHere is where the choice of tool becomes important in a way that isn’t always obvious.\n\nThe most powerful use case for in-house legal AI isn’t general research or document drafting. It’s a system that knows *your* contracts. Your standard agreements, your precedent bank, your historical advice, your GDPR documentation, your board minutes. A generative AI [deployed over your own document library](/blog/what-is-rag) can answer the CFO’s question about that haulage clause in seconds — not because it’s searched the internet, but because it’s read every version of that agreement you’ve ever signed.\n\nThat institutional knowledge retrieval is qualitatively different from anything a general-purpose AI tool provides. It’s also, for obvious reasons, something that requires complete confidence in where that data goes. The document library of a corporate legal function is among the most sensitive information a company holds: M&A materials, employment disputes, commercial negotiations, regulatory exposures. Feeding that into a public AI service — one whose terms permit training on inputs, or [disclosure to third parties](/blog/chatgpt-discovery-legal-risk) — is a risk that grows with the sophistication of the use case.\n\nJames, at his logistics company, could begin using a general AI tool to draft NDAs today. What he couldn’t do without a private deployment is ask it to cross-reference every active supplier contract for indemnification terms and flag the three that create the most exposure. That requires a system that works only with his data, on [infrastructure he controls](/blog/small-enough-to-trust).\n\nThat’s the version worth building toward.\n\n* * *\n\n*JD Fortress AI builds private, on-premises AI for in-house legal teams. If you’re interested in what a system trained on your own documents and precedents could look like — get in touch for a no-obligation conversation.*","src/content/blog/gen-ai-for-in-house-lawyers.md","0e6cd8d2dbb6f4ab",{html:134,metadata:135},"<p>James is head of legal at a 300-person logistics company in Birmingham. He has one qualified solicitor working under him, a part-time paralegal, and somewhere in the region of four hundred live commercial relationships to manage at any one time. His Monday mornings start with a stack of contract redlines from procurement, a question from HR about a disciplinary procedure, and an email from the CFO asking whether a clause in their main haulage agreement actually means what the finance team thinks it means.</p>\n<p>He isn’t unusual. The in-house legal function at most mid-sized UK businesses looks something like this: a small team, significant exposure, constant pressure to clear the queue. What varies is how long each task takes — and that’s precisely where generative AI is beginning to make a meaningful difference.</p>\n<h2 id=\"first-pass-work-is-the-biggest-drain-on-in-house-time\">First-pass work is the biggest drain on in-house time</h2>\n<p>The most time-consuming part of in-house legal work often isn’t the hard thinking. It’s the scaffolding that has to happen before the hard thinking can begin. Reading a 45-page supplier agreement to flag the clauses that matter. Producing a first draft of an NDA from scratch. Pulling together the research memo that lets you give an opinion on a regulatory question you haven’t encountered before. Translating a board resolution into the right board minutes format.</p>\n<p>None of this requires a qualified solicitor in the way that a nuanced commercial negotiation does. But it consumes qualified-solicitor hours at a significant rate.</p>\n<p>A well-deployed generative AI system can perform a first-pass review of a standard commercial contract in minutes, flagging non-standard clauses, identifying missing provisions, and summarising the document’s key obligations. The lawyer still makes every call — but they’re making those calls from an annotated starting position rather than a blank read-through. The research memo drafts itself from the AI’s synthesis of the relevant material; the lawyer checks it, adjusts it, and applies their judgment to the specific facts. The NDA starts at ninety percent rather than zero.</p>\n<p>According to the Association of Corporate Counsel’s latest survey, 91% of legal professionals who have adopted generative AI cite efficiency as its primary benefit. That figure isn’t surprising — but what’s notable is how quickly adoption has accelerated. In 2024, 23% of in-house counsel reported active use of GenAI tools. By 2025, that figure had more than doubled to 52%. Something shifted: the tools became good enough that the professional case for using them became difficult to ignore.</p>\n<h2 id=\"the-research-question-that-used-to-cost-you-an-external-brief\">The research question that used to cost you an external brief</h2>\n<p>There’s a particular kind of question that in-house teams regularly face: specific enough to need real legal analysis, general enough that it shouldn’t require a three-day brief from external counsel at £350 per hour.</p>\n<p>Employment law queries are a good example. So are data protection questions, supply chain contract disputes, and questions about implied terms in service agreements. These are areas where a competent in-house lawyer can give sound advice — but producing that advice historically required either significant research time or a call to an expensive specialist.</p>\n<p>Generative AI changes the economics of this considerably. A system trained on the relevant legal material — UK employment law, GDPR guidance, commercial contract principles — can surface the applicable framework, the key cases, and the relevant regulatory positions in the time it takes to write the question. The lawyer still provides the legal advice; the AI provides the reference architecture that would previously have taken an afternoon to assemble.</p>\n<p>The ACC survey found that 64% of in-house counsel expect generative AI to reduce their reliance on <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">external law firms</a>. That’s not wishful thinking — it reflects a real shift in what an in-house team of two or three people can credibly handle without outsourcing.</p>\n<h2 id=\"the-private-advantage--why-which-ai-matters-more-than-whether-ai\">The private advantage — why “which AI” matters more than “whether AI”</h2>\n<p>Here is where the choice of tool becomes important in a way that isn’t always obvious.</p>\n<p>The most powerful use case for in-house legal AI isn’t general research or document drafting. It’s a system that knows <em>your</em> contracts. Your standard agreements, your precedent bank, your historical advice, your GDPR documentation, your board minutes. A generative AI <a href=\"/blog/what-is-rag\">deployed over your own document library</a> can answer the CFO’s question about that haulage clause in seconds — not because it’s searched the internet, but because it’s read every version of that agreement you’ve ever signed.</p>\n<p>That institutional knowledge retrieval is qualitatively different from anything a general-purpose AI tool provides. It’s also, for obvious reasons, something that requires complete confidence in where that data goes. The document library of a corporate legal function is among the most sensitive information a company holds: M&#x26;A materials, employment disputes, commercial negotiations, regulatory exposures. Feeding that into a public AI service — one whose terms permit training on inputs, or <a href=\"/blog/chatgpt-discovery-legal-risk\">disclosure to third parties</a> — is a risk that grows with the sophistication of the use case.</p>\n<p>James, at his logistics company, could begin using a general AI tool to draft NDAs today. What he couldn’t do without a private deployment is ask it to cross-reference every active supplier contract for indemnification terms and flag the three that create the most exposure. That requires a system that works only with his data, on <a href=\"/blog/small-enough-to-trust\">infrastructure he controls</a>.</p>\n<p>That’s the version worth building toward.</p>\n<hr>\n<p><em>JD Fortress AI builds private, on-premises AI for in-house legal teams. If you’re interested in what a system trained on your own documents and precedents could look like — get in touch for a no-obligation conversation.</em></p>",{headings:136,localImagePaths:146,remoteImagePaths:147,frontmatter:148,imagePaths:150},[137,140,143],{depth:26,slug:138,text:139},"first-pass-work-is-the-biggest-drain-on-in-house-time","First-pass work is the biggest drain on in-house time",{depth:26,slug:141,text:142},"the-research-question-that-used-to-cost-you-an-external-brief","The research question that used to cost you an external brief",{depth:26,slug:144,text:145},"the-private-advantage--why-which-ai-matters-more-than-whether-ai","The private advantage — why “which AI” matters more than “whether AI”",[],[],{title:127,date:149,excerpt:129,author:17},["Date","2026-01-20T00:00:00.000Z"],[],"notebooklm-lawyer-lockout-warning",{id:151,data:153,body:157,filePath:158,digest:159,rendered:160},{title:154,date:155,excerpt:156,author:17},"The Off Switch You Don't Control",["Date","2026-03-06T00:00:00.000Z"],"A lawyer lost access to his Gmail, photos, and phone number after uploading lawful case files to Google's NotebookLM. The implications for UK legal professionals are worth sitting with.","On the morning of Saturday, 14 February, Brian Chase — adjunct law professor and managing director of digital forensics at ArcherHall — uploaded a set of text-only law enforcement reports to Google’s NotebookLM. He was working on a criminal case. The reports referenced child sexual abuse material because the defendant had been charged with possessing it. No images. No video. Just the kind of document that lands on a criminal defence lawyer’s desk as a matter of routine.\n\nWithin seconds, Google’s automated systems flagged a terms of service violation.\n\nChase deleted the upload the same day. On Monday morning, he woke up signed out of every Google service he used. His Gmail was inaccessible. His Google Voice number — the one attached to his professional identity — was gone. So were his photos and contacts. “Nothing I uploaded was illegal,” he wrote on LinkedIn. “Nothing I did violated the attorney ethical rules. But Google flagged it anyway, and there is very little recourse once that happens.” His account was eventually restored on Tuesday, after an automated appeal process in which there was no human being to speak to. Google did not respond to press enquiries.\n\n## The scanner that doesn’t read context\n\nGoogle’s free NotebookLM tier explicitly states that it may process uploaded data “to prevent fraud, abuse, and technical issues.” That is the line that matters. It isn’t that Google is reading documents out of curiosity — it’s that automated systems are scanning everything that passes through their servers, applying pattern-matching logic that cannot distinguish between a criminal defence lawyer handling case evidence and an individual in possession of prohibited material.\n\nThe scanner doesn’t know that the flagged text was a police report. It doesn’t know that the uploader had professional and ethical obligations to review that material. It knows only that certain strings matched a policy filter, and that the filter is applied account-wide. One document. One flag. Every linked service, suspended.\n\nThat is the architecture. Not a bug — the design.\n\n## The risk that nobody talks about\n\nMost discussion about AI tools and client confidentiality focuses on the training question: will uploaded documents be used to improve the model? That is a real concern, and one that the [CLOUD Act and its reach into US-hosted services](/blog/why-law-firms-cant-afford-cloud-ai) makes more acute. But it is not the most immediate risk. Chase’s case illustrates a different vulnerability entirely.\n\nWhen your AI tool runs on a third-party platform, that platform controls the off switch. It controls what content is permitted, what triggers enforcement, and what access you retain when enforcement fires. The appeals process is whatever the platform has chosen to build — which may or may not include a human being capable of understanding professional context. For Chase, it was a 48-hour automated loop that eventually produced a restoration notice. For a solicitor with a court deadline, those 48 hours are not recoverable.\n\nThe same story notes that both ChatGPT and NotebookLM refused to summarise publicly available Justice Department documents from the Epstein case — documents that journalists and lawyers access routinely. Two AI tools based in China summarised them without hesitation. That asymmetry tells you something about where content moderation policy ends and something else begins. As [legal discovery obligations around AI-held data](/blog/chatgpt-discovery-legal-risk) become better understood, the behaviour of these platforms under pressure deserves more scrutiny than it currently receives.\n\n## What the SRA expects — and what it can’t anticipate\n\nThe Solicitors Regulation Authority has published guidance making clear that solicitors must “undertake due diligence” before using any AI platform with client data, and must satisfy themselves that use of the platform is consistent with their obligations under SRA Principle 6 — the duty to keep client affairs confidential.\n\nWhat that guidance cannot anticipate is this: what happens when the lawful professional work a solicitor is doing is misidentified by an automated content moderation system? Chase wasn’t uploading client data in any conventional sense. He was working with case evidence in the course of lawful legal work, and he still lost access to his professional communications infrastructure for the best part of two working days.\n\nA framework that [asks whether AI use constitutes a disclosure to a third party](/blog/ai-third-party-privilege-ruling) rightly focusses on intentional sharing. The Google incident introduces a different question — one the regulators haven’t quite caught up with. What is the professional status of an account lockout? What are your obligations to a client whose matter is sitting in a disabled inbox?\n\n## Owning the off switch\n\nOn-premise AI doesn’t have a content moderation system that can remove your access. The model runs on hardware you control, inside a network you control, under policies you set. There is no automated scanner reviewing uploaded documents for compliance with rules written by a platform’s legal team in California. There is no account-wide enforcement action that can take your email infrastructure down with it.\n\nThe Brian Chase incident is an unusually clean illustration of a structural vulnerability that affects any legal professional who relies on Big Tech AI platforms for sensitive work. The question isn’t whether you trust Google’s intentions. The question is whether you are comfortable with them holding the off switch — and what you’d tell a client if they flipped it.\n\n* * *\n\n*JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.*","src/content/blog/notebooklm-lawyer-lockout-warning.md","49c024c0b759fee3",{html:161,metadata:162},"<p>On the morning of Saturday, 14 February, Brian Chase — adjunct law professor and managing director of digital forensics at ArcherHall — uploaded a set of text-only law enforcement reports to Google’s NotebookLM. He was working on a criminal case. The reports referenced child sexual abuse material because the defendant had been charged with possessing it. No images. No video. Just the kind of document that lands on a criminal defence lawyer’s desk as a matter of routine.</p>\n<p>Within seconds, Google’s automated systems flagged a terms of service violation.</p>\n<p>Chase deleted the upload the same day. On Monday morning, he woke up signed out of every Google service he used. His Gmail was inaccessible. His Google Voice number — the one attached to his professional identity — was gone. So were his photos and contacts. “Nothing I uploaded was illegal,” he wrote on LinkedIn. “Nothing I did violated the attorney ethical rules. But Google flagged it anyway, and there is very little recourse once that happens.” His account was eventually restored on Tuesday, after an automated appeal process in which there was no human being to speak to. Google did not respond to press enquiries.</p>\n<h2 id=\"the-scanner-that-doesnt-read-context\">The scanner that doesn’t read context</h2>\n<p>Google’s free NotebookLM tier explicitly states that it may process uploaded data “to prevent fraud, abuse, and technical issues.” That is the line that matters. It isn’t that Google is reading documents out of curiosity — it’s that automated systems are scanning everything that passes through their servers, applying pattern-matching logic that cannot distinguish between a criminal defence lawyer handling case evidence and an individual in possession of prohibited material.</p>\n<p>The scanner doesn’t know that the flagged text was a police report. It doesn’t know that the uploader had professional and ethical obligations to review that material. It knows only that certain strings matched a policy filter, and that the filter is applied account-wide. One document. One flag. Every linked service, suspended.</p>\n<p>That is the architecture. Not a bug — the design.</p>\n<h2 id=\"the-risk-that-nobody-talks-about\">The risk that nobody talks about</h2>\n<p>Most discussion about AI tools and client confidentiality focuses on the training question: will uploaded documents be used to improve the model? That is a real concern, and one that the <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">CLOUD Act and its reach into US-hosted services</a> makes more acute. But it is not the most immediate risk. Chase’s case illustrates a different vulnerability entirely.</p>\n<p>When your AI tool runs on a third-party platform, that platform controls the off switch. It controls what content is permitted, what triggers enforcement, and what access you retain when enforcement fires. The appeals process is whatever the platform has chosen to build — which may or may not include a human being capable of understanding professional context. For Chase, it was a 48-hour automated loop that eventually produced a restoration notice. For a solicitor with a court deadline, those 48 hours are not recoverable.</p>\n<p>The same story notes that both ChatGPT and NotebookLM refused to summarise publicly available Justice Department documents from the Epstein case — documents that journalists and lawyers access routinely. Two AI tools based in China summarised them without hesitation. That asymmetry tells you something about where content moderation policy ends and something else begins. As <a href=\"/blog/chatgpt-discovery-legal-risk\">legal discovery obligations around AI-held data</a> become better understood, the behaviour of these platforms under pressure deserves more scrutiny than it currently receives.</p>\n<h2 id=\"what-the-sra-expects--and-what-it-cant-anticipate\">What the SRA expects — and what it can’t anticipate</h2>\n<p>The Solicitors Regulation Authority has published guidance making clear that solicitors must “undertake due diligence” before using any AI platform with client data, and must satisfy themselves that use of the platform is consistent with their obligations under SRA Principle 6 — the duty to keep client affairs confidential.</p>\n<p>What that guidance cannot anticipate is this: what happens when the lawful professional work a solicitor is doing is misidentified by an automated content moderation system? Chase wasn’t uploading client data in any conventional sense. He was working with case evidence in the course of lawful legal work, and he still lost access to his professional communications infrastructure for the best part of two working days.</p>\n<p>A framework that <a href=\"/blog/ai-third-party-privilege-ruling\">asks whether AI use constitutes a disclosure to a third party</a> rightly focusses on intentional sharing. The Google incident introduces a different question — one the regulators haven’t quite caught up with. What is the professional status of an account lockout? What are your obligations to a client whose matter is sitting in a disabled inbox?</p>\n<h2 id=\"owning-the-off-switch\">Owning the off switch</h2>\n<p>On-premise AI doesn’t have a content moderation system that can remove your access. The model runs on hardware you control, inside a network you control, under policies you set. There is no automated scanner reviewing uploaded documents for compliance with rules written by a platform’s legal team in California. There is no account-wide enforcement action that can take your email infrastructure down with it.</p>\n<p>The Brian Chase incident is an unusually clean illustration of a structural vulnerability that affects any legal professional who relies on Big Tech AI platforms for sensitive work. The question isn’t whether you trust Google’s intentions. The question is whether you are comfortable with them holding the off switch — and what you’d tell a client if they flipped it.</p>\n<hr>\n<p><em>JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.</em></p>",{headings:163,localImagePaths:176,remoteImagePaths:177,frontmatter:178,imagePaths:180},[164,167,170,173],{depth:26,slug:165,text:166},"the-scanner-that-doesnt-read-context","The scanner that doesn’t read context",{depth:26,slug:168,text:169},"the-risk-that-nobody-talks-about","The risk that nobody talks about",{depth:26,slug:171,text:172},"what-the-sra-expects--and-what-it-cant-anticipate","What the SRA expects — and what it can’t anticipate",{depth:26,slug:174,text:175},"owning-the-off-switch","Owning the off switch",[],[],{title:154,date:179,excerpt:156,author:17},["Date","2026-03-06T00:00:00.000Z"],[],"on-cocounsel",{id:181,data:183,body:187,filePath:188,digest:189,rendered:190},{title:184,date:185,excerpt:186,author:17},"Our Thoughts on CoCounsel—the “Industry-leading AI assistant for professionals”",["Date","2026-02-24T00:00:00.000Z"],"CoCounsel from Thomson Reuters promises serious productivity gains for legal work, built on decades of trusted content. But for many UK High Street firms, the cloud-based architecture still raises hard questions about client confidentiality under SRA rules. Here’s what we’ve found.","Thomson Reuters has been a fixture in legal research for generations, so when their CoCounsel platform started getting serious attention as a generative AI assistant for lawyers, we paid attention.\n\nFirst, a quick note on Practical Law, since CoCounsel builds heavily on it. Practical Law is essentially a vast, curated know-how resource: over 118,000 documents across practice areas, including standard clauses, drafting notes, checklists, and plain-language explanations maintained by hundreds of experienced attorney-editors. It’s designed to help practitioners (especially in firms without massive libraries) draft faster, understand unfamiliar areas, and stay current on changes. For UK firms, it offers solid coverage of English/Welsh law alongside global materials. CoCounsel integrates this content directly, grounding its AI outputs in those authoritative sources rather than pure open-web generation.\nFrom what we’ve gathered talking to contacts and reviewing public materials, the main attractions for High Street and mid-sized firms seem to be:\n\nSpeed on repetitive tasks: document review, contract drafting, summarising large sets of materials, spotting issues in litigation docs, or pulling together research plans.\n\nAgentic workflows that chain steps (research → analysis → draft) without constant prompting.\nFamiliar integration with tools like Microsoft 365 or document management systems.\n\nUsers report cutting document review or drafting time significantly (Thomson Reuters cites figures like 2-3x faster in some cases), and the grounding in Westlaw/Practical Law content helps reduce hallucinations compared to generic tools.\n\nIntroductory/ongoing pricing isn’t publicly listed in detail—you contact sales for tailored quotes, often tied to existing Thomson Reuters subscriptions (Westlaw, Practical Law bundles). \n\nSome older references point to around £250 per user/month for certain tiers with multi-year commitments, but enterprise deals vary widely. It’s positioned as premium, reflecting the depth of content and support.\n\nBut here’s where we keep coming back to the same question, especially after conversations with compliance leads and partners at smaller UK practices: how does this square with absolute client confidentiality?\n\nCoCounsel is cloud-hosted. When you upload documents or run queries involving client matters, that data transits to Thomson Reuters’ infrastructure and, for generative steps, to third-party LLM providers (historically OpenAI’s GPT series; more recently they’ve tested custom builds on models like o1-mini, alongside Google and Anthropic options). Thomson Reuters is clear about protections: requests are anonymised under their identity (not yours), use zero-retention APIs (no storage by the LLM provider), data is encrypted in transit (TLS 1.2) and at rest (AES-256), and contractual terms prohibit third parties from using customer data to train models. They also highlight SOC 2, ISO 27001, and responsible AI practices.\n\nThat’s strong on paper—better than many consumer-grade tools. Yet for a solicitor bound by [SRA Principle 6](/blog/why-law-firms-cant-afford-cloud-ai) (absolute duty of confidentiality) and the Code of Conduct, the fact remains: [sensitive client information leaves your controlled environment](/blog/notebooklm-lawyer-lockout-warning) and passes through external processors, even if anonymised and not retained. The SRA’s AI guidance (still evolving) stresses assessing risks of disclosure, and many firms we speak to hesitate at that point.\n\nRedaction or sanitisation before upload? Public materials don’t detail automated mechanisms (e.g., no mention of regex patterns, entity recognition, or secondary LLMs for scrubbing). If firms handle that manually, it adds friction and risk of human error. If automated within CoCounsel, it still involves sending the raw document upstream first.\n\nWe get why CoCounsel appeals—it’s polished, content-rich, and backed by a giant in the space. For firms already deep in the Thomson Reuters ecosystem, the controls may feel sufficient.\n\nBut for High Street practices handling personal injury, family, conveyancing, or probate—where a single inadvertent disclosure could end careers—we can’t shake the concern. Why accept any external transit when the alternative exists?\n\nThat’s why we focus on fully offline, on-premises deployments. Our systems run entirely on your hardware or private VPC/air-gapped networks: no data ever leaves your perimeter, no third-party APIs, no anonymisation gymnastics required. Custom [RAG pipelines](/blog/what-is-rag) pull answers exclusively from your documents and knowledge base, and we can layer on [agentic behaviours](/blog/what-is-openclaw) (proactive monitoring, task execution, autonomous drafting/responses) inspired by open frameworks—all local, all under your control.\n\nThe efficiency is comparable (research, summarisation, first drafts, compliance checks), but the risk profile is fundamentally different. No CLOUD Act exposure, no evolving third-party terms, no “trust us” on zero-retention.\n\nIf you’re weighing tools like CoCounsel and wondering whether the convenience outweighs the confidentiality questions, we’re happy to talk through what a truly private setup looks like for your firm—no hard sell, just practical comparison.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.*","src/content/blog/on-cocounsel.md","fee96e425bb2d20b",{html:191,metadata:192},"<p>Thomson Reuters has been a fixture in legal research for generations, so when their CoCounsel platform started getting serious attention as a generative AI assistant for lawyers, we paid attention.</p>\n<p>First, a quick note on Practical Law, since CoCounsel builds heavily on it. Practical Law is essentially a vast, curated know-how resource: over 118,000 documents across practice areas, including standard clauses, drafting notes, checklists, and plain-language explanations maintained by hundreds of experienced attorney-editors. It’s designed to help practitioners (especially in firms without massive libraries) draft faster, understand unfamiliar areas, and stay current on changes. For UK firms, it offers solid coverage of English/Welsh law alongside global materials. CoCounsel integrates this content directly, grounding its AI outputs in those authoritative sources rather than pure open-web generation.\nFrom what we’ve gathered talking to contacts and reviewing public materials, the main attractions for High Street and mid-sized firms seem to be:</p>\n<p>Speed on repetitive tasks: document review, contract drafting, summarising large sets of materials, spotting issues in litigation docs, or pulling together research plans.</p>\n<p>Agentic workflows that chain steps (research → analysis → draft) without constant prompting.\nFamiliar integration with tools like Microsoft 365 or document management systems.</p>\n<p>Users report cutting document review or drafting time significantly (Thomson Reuters cites figures like 2-3x faster in some cases), and the grounding in Westlaw/Practical Law content helps reduce hallucinations compared to generic tools.</p>\n<p>Introductory/ongoing pricing isn’t publicly listed in detail—you contact sales for tailored quotes, often tied to existing Thomson Reuters subscriptions (Westlaw, Practical Law bundles).</p>\n<p>Some older references point to around £250 per user/month for certain tiers with multi-year commitments, but enterprise deals vary widely. It’s positioned as premium, reflecting the depth of content and support.</p>\n<p>But here’s where we keep coming back to the same question, especially after conversations with compliance leads and partners at smaller UK practices: how does this square with absolute client confidentiality?</p>\n<p>CoCounsel is cloud-hosted. When you upload documents or run queries involving client matters, that data transits to Thomson Reuters’ infrastructure and, for generative steps, to third-party LLM providers (historically OpenAI’s GPT series; more recently they’ve tested custom builds on models like o1-mini, alongside Google and Anthropic options). Thomson Reuters is clear about protections: requests are anonymised under their identity (not yours), use zero-retention APIs (no storage by the LLM provider), data is encrypted in transit (TLS 1.2) and at rest (AES-256), and contractual terms prohibit third parties from using customer data to train models. They also highlight SOC 2, ISO 27001, and responsible AI practices.</p>\n<p>That’s strong on paper—better than many consumer-grade tools. Yet for a solicitor bound by <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">SRA Principle 6</a> (absolute duty of confidentiality) and the Code of Conduct, the fact remains: <a href=\"/blog/notebooklm-lawyer-lockout-warning\">sensitive client information leaves your controlled environment</a> and passes through external processors, even if anonymised and not retained. The SRA’s AI guidance (still evolving) stresses assessing risks of disclosure, and many firms we speak to hesitate at that point.</p>\n<p>Redaction or sanitisation before upload? Public materials don’t detail automated mechanisms (e.g., no mention of regex patterns, entity recognition, or secondary LLMs for scrubbing). If firms handle that manually, it adds friction and risk of human error. If automated within CoCounsel, it still involves sending the raw document upstream first.</p>\n<p>We get why CoCounsel appeals—it’s polished, content-rich, and backed by a giant in the space. For firms already deep in the Thomson Reuters ecosystem, the controls may feel sufficient.</p>\n<p>But for High Street practices handling personal injury, family, conveyancing, or probate—where a single inadvertent disclosure could end careers—we can’t shake the concern. Why accept any external transit when the alternative exists?</p>\n<p>That’s why we focus on fully offline, on-premises deployments. Our systems run entirely on your hardware or private VPC/air-gapped networks: no data ever leaves your perimeter, no third-party APIs, no anonymisation gymnastics required. Custom <a href=\"/blog/what-is-rag\">RAG pipelines</a> pull answers exclusively from your documents and knowledge base, and we can layer on <a href=\"/blog/what-is-openclaw\">agentic behaviours</a> (proactive monitoring, task execution, autonomous drafting/responses) inspired by open frameworks—all local, all under your control.</p>\n<p>The efficiency is comparable (research, summarisation, first drafts, compliance checks), but the risk profile is fundamentally different. No CLOUD Act exposure, no evolving third-party terms, no “trust us” on zero-retention.</p>\n<p>If you’re weighing tools like CoCounsel and wondering whether the convenience outweighs the confidentiality questions, we’re happy to talk through what a truly private setup looks like for your firm—no hard sell, just practical comparison.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion — no pitch, just practical talk.</em></p>",{headings:193,localImagePaths:194,remoteImagePaths:195,frontmatter:196,imagePaths:198},[],[],[],{title:184,date:197,excerpt:186,author:17},["Date","2026-02-24T00:00:00.000Z"],[],"ai-third-party-privilege-ruling",{id:199,data:201,body:205,filePath:206,digest:207,rendered:208},{title:202,date:203,excerpt:204,author:17},"When AI Becomes the Third Party: The US Ruling Every UK Lawyer Needs to Read",["Date","2026-03-03T00:00:00.000Z"],"A US federal court just stripped legal privilege from documents created in a public AI tool. The reasoning maps directly onto UK practice - and no one should wait for a domestic equivalent.","A few months ago, we started hearing a version of the same question from compliance leads at law firms across the south of England: \"If a fee earner types something into [ChatGPT](/blog/chatgpt-discovery-legal-risk) or Claude, and it relates to a client matter, where does that go?\" The honest answer was always: to a server owned by a US tech company, under terms that permit training, disclosure to regulators, and use in ways you haven't fully parsed. Uncomfortable — but, until recently, [still somewhat theoretical](/blog/notebooklm-lawyer-lockout-warning).\n\nOn 10 February 2026, it stopped being theoretical.\n\nJudge Jed Rakoff of the US District Court for the Southern District of New York ruled in *United States v. Heppner* that a criminal defendant could not claim attorney-client privilege over documents he had created using Anthropic's public version of Claude. The detail that should unsettle every lawyer: he had typed information received directly from his own legal counsel into the tool - intending, his defence argued, to consolidate his thoughts before speaking to his lawyers. It didn't matter. The court found that once privileged information passed through a public AI tool, the privilege was gone.\n\n## Why the terms of service killed the privilege\n\nThe court's reasoning wasn't complicated, and that's precisely what makes it significant.\n\nAttorney-client privilege requires, at its core, that communications remain confidential. Claude's standard consumer terms permit Anthropic to use inputs and outputs for model training, and to disclose user data to third parties and regulatory authorities. That language - buried in the terms most people click past - was enough for Judge Rakoff. A user who accepts those terms has no reasonable expectation of confidentiality. And without confidentiality, there is no privilege to protect.\n\nThe work-product doctrine didn't save the defendant either. That doctrine shields materials prepared by or at the direction of counsel in anticipation of litigation. Heppner had created the AI documents entirely on his own initiative - his legal team hadn't asked him to, and weren't involved in their creation. On both grounds, the court held, the documents were fair game for the government.\n\nHeppner's counsel from Quinn Emanuel argued that the documents should still be protected because they incorporated information their client had received during the course of the legal representation. Judge Rakoff saw it differently - and offered a blunt assessment that he saw \"remotely, any basis for any claim of attorney-client privilege\" as essentially non-existent.\n\n## The enterprise exception the court deliberately left open\n\nThis is where it gets instructive for legal professionals making decisions right now.\n\nJudge Rakoff's ruling was deliberately narrow: it concerned a consumer-grade, non-enterprise version of a public AI tool. Several leading US firms - Proskauer, Debevoise, A&O Shearman - writing in the immediate aftermath, noted that the court explicitly left open whether enterprise AI tools might support a different analysis. Enterprise deployments - those with contractual confidentiality protections, terms that exclude user inputs from model training, and clear restrictions on data disclosure - could, the reasoning implies, preserve the confidentiality that privilege requires.\n\nIn other words: the problem isn't AI. The problem is *public* AI, operating under consumer terms, touching information that your client, your regulator, and apparently now a federal judge expect to remain private.\n\nThat distinction matters enormously for how firms approach their technology choices. It's not a binary between \"use AI\" and \"preserve privilege.\" It's a question of *which* AI, under *which* terms, on *whose* infrastructure.\n\n## The UK parallel is closer than you'd think\n\nThis is a US ruling, applying US privilege law. But its underlying logic maps directly onto the landscape facing UK solicitors.\n\nLegal professional privilege in England and Wales operates on the same foundational principle: confidentiality. The [SRA's own risk outlook on AI](/blog/why-law-firms-cant-afford-cloud-ai) notes that \"firms adopting AI will need to make sure that their use of it protects confidentiality and legal privilege.\" Their compliance tips for solicitors urge due diligence on whether third-party AI platforms have been designed so that firms aren't in breach of their obligations. That's measured regulatory language for: if you can't trace where your clients' data goes, you're exposed.\n\nWe spoke recently with a fee earner at a small commercial property firm outside of London. She'd been using a general-purpose AI assistant to help draft correspondence - nothing dramatic, just time-saving. When we asked whether she'd reviewed the tool's data policies, she paused. \"I assumed the firm had done that. No one told me not to use it.\" Her firm had no AI use policy at all.\n\nThat scenario is now a privilege waiver waiting to happen.\n\nThe Heppner ruling won't bind UK courts. But the question it answers - can a public AI tool constitute a third-party disclosure that destroys legal privilege? - is one UK practitioners should be asking before a domestic court answers it for them.\n\nThe answer isn't to ban AI from legal work. It's to ensure that whatever AI your firm uses operates entirely within a controlled, [private environment](/blog/what-is-rag): one where data doesn't transit a public provider's servers, where terms don't permit training on your inputs, and where confidentiality is a structural guarantee rather than a contractual hope.\n\nIf you'd like to understand what that looks like in practice for a firm your size - no jargon, no hard sell - we're happy to talk it through.\n\n* * *\n\n*JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.*","src/content/blog/ai-third-party-privilege-ruling.md","1fa6961457040ecf",{html:209,metadata:210},"<p>A few months ago, we started hearing a version of the same question from compliance leads at law firms across the south of England: “If a fee earner types something into <a href=\"/blog/chatgpt-discovery-legal-risk\">ChatGPT</a> or Claude, and it relates to a client matter, where does that go?” The honest answer was always: to a server owned by a US tech company, under terms that permit training, disclosure to regulators, and use in ways you haven’t fully parsed. Uncomfortable — but, until recently, <a href=\"/blog/notebooklm-lawyer-lockout-warning\">still somewhat theoretical</a>.</p>\n<p>On 10 February 2026, it stopped being theoretical.</p>\n<p>Judge Jed Rakoff of the US District Court for the Southern District of New York ruled in <em>United States v. Heppner</em> that a criminal defendant could not claim attorney-client privilege over documents he had created using Anthropic’s public version of Claude. The detail that should unsettle every lawyer: he had typed information received directly from his own legal counsel into the tool - intending, his defence argued, to consolidate his thoughts before speaking to his lawyers. It didn’t matter. The court found that once privileged information passed through a public AI tool, the privilege was gone.</p>\n<h2 id=\"why-the-terms-of-service-killed-the-privilege\">Why the terms of service killed the privilege</h2>\n<p>The court’s reasoning wasn’t complicated, and that’s precisely what makes it significant.</p>\n<p>Attorney-client privilege requires, at its core, that communications remain confidential. Claude’s standard consumer terms permit Anthropic to use inputs and outputs for model training, and to disclose user data to third parties and regulatory authorities. That language - buried in the terms most people click past - was enough for Judge Rakoff. A user who accepts those terms has no reasonable expectation of confidentiality. And without confidentiality, there is no privilege to protect.</p>\n<p>The work-product doctrine didn’t save the defendant either. That doctrine shields materials prepared by or at the direction of counsel in anticipation of litigation. Heppner had created the AI documents entirely on his own initiative - his legal team hadn’t asked him to, and weren’t involved in their creation. On both grounds, the court held, the documents were fair game for the government.</p>\n<p>Heppner’s counsel from Quinn Emanuel argued that the documents should still be protected because they incorporated information their client had received during the course of the legal representation. Judge Rakoff saw it differently - and offered a blunt assessment that he saw “remotely, any basis for any claim of attorney-client privilege” as essentially non-existent.</p>\n<h2 id=\"the-enterprise-exception-the-court-deliberately-left-open\">The enterprise exception the court deliberately left open</h2>\n<p>This is where it gets instructive for legal professionals making decisions right now.</p>\n<p>Judge Rakoff’s ruling was deliberately narrow: it concerned a consumer-grade, non-enterprise version of a public AI tool. Several leading US firms - Proskauer, Debevoise, A&#x26;O Shearman - writing in the immediate aftermath, noted that the court explicitly left open whether enterprise AI tools might support a different analysis. Enterprise deployments - those with contractual confidentiality protections, terms that exclude user inputs from model training, and clear restrictions on data disclosure - could, the reasoning implies, preserve the confidentiality that privilege requires.</p>\n<p>In other words: the problem isn’t AI. The problem is <em>public</em> AI, operating under consumer terms, touching information that your client, your regulator, and apparently now a federal judge expect to remain private.</p>\n<p>That distinction matters enormously for how firms approach their technology choices. It’s not a binary between “use AI” and “preserve privilege.” It’s a question of <em>which</em> AI, under <em>which</em> terms, on <em>whose</em> infrastructure.</p>\n<h2 id=\"the-uk-parallel-is-closer-than-youd-think\">The UK parallel is closer than you’d think</h2>\n<p>This is a US ruling, applying US privilege law. But its underlying logic maps directly onto the landscape facing UK solicitors.</p>\n<p>Legal professional privilege in England and Wales operates on the same foundational principle: confidentiality. The <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">SRA’s own risk outlook on AI</a> notes that “firms adopting AI will need to make sure that their use of it protects confidentiality and legal privilege.” Their compliance tips for solicitors urge due diligence on whether third-party AI platforms have been designed so that firms aren’t in breach of their obligations. That’s measured regulatory language for: if you can’t trace where your clients’ data goes, you’re exposed.</p>\n<p>We spoke recently with a fee earner at a small commercial property firm outside of London. She’d been using a general-purpose AI assistant to help draft correspondence - nothing dramatic, just time-saving. When we asked whether she’d reviewed the tool’s data policies, she paused. “I assumed the firm had done that. No one told me not to use it.” Her firm had no AI use policy at all.</p>\n<p>That scenario is now a privilege waiver waiting to happen.</p>\n<p>The Heppner ruling won’t bind UK courts. But the question it answers - can a public AI tool constitute a third-party disclosure that destroys legal privilege? - is one UK practitioners should be asking before a domestic court answers it for them.</p>\n<p>The answer isn’t to ban AI from legal work. It’s to ensure that whatever AI your firm uses operates entirely within a controlled, <a href=\"/blog/what-is-rag\">private environment</a>: one where data doesn’t transit a public provider’s servers, where terms don’t permit training on your inputs, and where confidentiality is a structural guarantee rather than a contractual hope.</p>\n<p>If you’d like to understand what that looks like in practice for a firm your size - no jargon, no hard sell - we’re happy to talk it through.</p>\n<hr>\n<p><em>JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.</em></p>",{headings:211,localImagePaths:221,remoteImagePaths:222,frontmatter:223,imagePaths:225},[212,215,218],{depth:26,slug:213,text:214},"why-the-terms-of-service-killed-the-privilege","Why the terms of service killed the privilege",{depth:26,slug:216,text:217},"the-enterprise-exception-the-court-deliberately-left-open","The enterprise exception the court deliberately left open",{depth:26,slug:219,text:220},"the-uk-parallel-is-closer-than-youd-think","The UK parallel is closer than you’d think",[],[],{title:202,date:224,excerpt:204,author:17},["Date","2026-03-03T00:00:00.000Z"],[],"small-enough-to-trust",{id:226,data:228,body:232,filePath:233,digest:234,rendered:235},{title:229,date:230,excerpt:231,author:17},"Small Enough to Trust: What NanoClaw Gets Right About Private AI",["Date","2026-03-10T00:00:00.000Z"],"Most enterprise AI is a black box you're asked to simply trust. A project called NanoClaw takes a different view - and it points toward something important about how serious AI deployments should be built.","There's a tension at the heart of enterprise software that rarely gets named directly. Vendors add features because features are how you justify a price increase and win deals. Buyers accept features because a longer spec sheet feels like reduced risk. The result, across decades of enterprise technology, is systems that are vastly more capable on paper than any single organisation needs - and vastly more opaque than any IT team can confidently audit. For most categories of software, the trade-off is acceptable. For AI systems that have access to your most sensitive data, it isn't.\n\nA project in the open-source AI community has been making this argument more concisely than anyone else. NanoClaw, built by developer Gabriel C. and quietly gaining serious attention - it was recently featured on Docker's engineering blog - takes direct aim at the complexity trap. Its README opens with a pointed observation: its better-known counterpart runs 52 modules, 8 configuration management files, and 45-plus dependencies. NanoClaw delivers the same core functionality in roughly 500 lines of code. A codebase, the author notes, you can understand in eight minutes.\n\nThat's not a boast about minimalism for its own sake. It's a security claim.\n\n## Security by isolation, not by policy\n\nThe conventional approach to securing AI systems is application-level: allowlists that restrict what the agent can do, pairing codes that control who can interact with it, permission tiers that govern which features are accessible. These are reasonable controls. They are also software controls - which means they live in the same process as everything else, sharing the same memory, subject to the same bugs and edge cases.\n\nNanoClaw takes a different approach. Agents run in actual Linux containers - OS-level isolation, not application-level checks. The agent can only see what has been explicitly mounted into its environment. Bash access, which sounds alarming, is safe because the commands execute inside the container, not on the host machine. The security model isn't \"we've written rules that prevent bad things\" - it's \"the bad things physically cannot reach the data they'd need to cause harm.\"\n\nFor organisations in regulated sectors - law firms, financial services, healthcare - this distinction matters enormously. [Application-level security can be circumvented](/blog/notebooklm-lawyer-lockout-warning). Architectural security requires defeating the architecture. When a client asks us \"how do we know the AI can't access things it shouldn't?\", an answer grounded in OS-level isolation is a fundamentally different conversation from one grounded in permission settings.\n\n## A system that grows where you need it\n\nThe other half of NanoClaw's philosophy - and the part we find most compelling - is what it calls skills over features.\n\nMost software platforms grow by addition. The vendor ships updates, capabilities accumulate, and your installation gradually becomes a superset of what you actually use. The modules you don't use still exist, still run, still represent potential exposure. The configuration options you don't touch still need to be understood by whoever is responsible for keeping the system secure.\n\nNanoClaw inverts this. The base system is deliberately minimal. When you need a new capability - Gmail integration, a new communication channel, scheduled briefings - you add a skill: a set of instructions that transforms your installation to include exactly that capability. Not a toggle in a settings panel. An actual change to the code, scoped precisely to what you need, that you can read and understand before it goes live.\n\nThe practical implication for a business is significant. A deployment that starts with document review and later needs to add regulatory compliance queries, or cross-reference supplier contracts, or monitor a specific set of public filings - each of those additions is deliberate, auditable, and reversible. The system grows with the business's actual needs rather than arriving pre-loaded with capabilities that may or may not be relevant. There's no inherited feature set to worry about. There's no \"we're not sure exactly what that module does\" answer to give an auditor.\n\n## The harness matters as much as the model\n\nOne line in NanoClaw's README deserves to be quoted in full: \"A bad harness makes even smart models seem dumb, a good harness gives them superpowers.\"\n\nThis is the insight that separates thoughtful AI deployment from plugging an API key into a generic interface and measuring the results. The underlying model - however capable - performs within the constraints of the infrastructure around it. How context is structured and maintained. How tasks are isolated. How results are routed back. How the system handles the edge cases that the demos never show. These are harness decisions, and they determine whether the output is reliable, consistent, and useful under real working conditions rather than controlled demonstrations.\n\nNanoClaw makes a deliberate choice: run directly on the Claude Agent SDK, the same infrastructure that powers Claude Code, rather than abstracting away from it. The harness is as close to the model as it can be. That's a principled engineering decision, and it's one we recognise.\n\nThe AI projects that will endure in [regulated businesses](/blog/why-law-firms-cant-afford-cloud-ai) aren't the ones that offered the most features at the point of sale. They're the ones where someone can still explain, years later, exactly what is running, what it can see, and why the output should be trusted. NanoClaw builds toward that standard. So do we.\n\n* * *\n\n*JD Fortress AI deploys secure, on-premises AI for businesses across regulated sectors in the UK. Get in touch for a confidential discussion - no pitch, just practical talk.*","src/content/blog/small-enough-to-trust.md","5e878d89aea2cbfa",{html:236,metadata:237},"<p>There’s a tension at the heart of enterprise software that rarely gets named directly. Vendors add features because features are how you justify a price increase and win deals. Buyers accept features because a longer spec sheet feels like reduced risk. The result, across decades of enterprise technology, is systems that are vastly more capable on paper than any single organisation needs - and vastly more opaque than any IT team can confidently audit. For most categories of software, the trade-off is acceptable. For AI systems that have access to your most sensitive data, it isn’t.</p>\n<p>A project in the open-source AI community has been making this argument more concisely than anyone else. NanoClaw, built by developer Gabriel C. and quietly gaining serious attention - it was recently featured on Docker’s engineering blog - takes direct aim at the complexity trap. Its README opens with a pointed observation: its better-known counterpart runs 52 modules, 8 configuration management files, and 45-plus dependencies. NanoClaw delivers the same core functionality in roughly 500 lines of code. A codebase, the author notes, you can understand in eight minutes.</p>\n<p>That’s not a boast about minimalism for its own sake. It’s a security claim.</p>\n<h2 id=\"security-by-isolation-not-by-policy\">Security by isolation, not by policy</h2>\n<p>The conventional approach to securing AI systems is application-level: allowlists that restrict what the agent can do, pairing codes that control who can interact with it, permission tiers that govern which features are accessible. These are reasonable controls. They are also software controls - which means they live in the same process as everything else, sharing the same memory, subject to the same bugs and edge cases.</p>\n<p>NanoClaw takes a different approach. Agents run in actual Linux containers - OS-level isolation, not application-level checks. The agent can only see what has been explicitly mounted into its environment. Bash access, which sounds alarming, is safe because the commands execute inside the container, not on the host machine. The security model isn’t “we’ve written rules that prevent bad things” - it’s “the bad things physically cannot reach the data they’d need to cause harm.”</p>\n<p>For organisations in regulated sectors - law firms, financial services, healthcare - this distinction matters enormously. <a href=\"/blog/notebooklm-lawyer-lockout-warning\">Application-level security can be circumvented</a>. Architectural security requires defeating the architecture. When a client asks us “how do we know the AI can’t access things it shouldn’t?”, an answer grounded in OS-level isolation is a fundamentally different conversation from one grounded in permission settings.</p>\n<h2 id=\"a-system-that-grows-where-you-need-it\">A system that grows where you need it</h2>\n<p>The other half of NanoClaw’s philosophy - and the part we find most compelling - is what it calls skills over features.</p>\n<p>Most software platforms grow by addition. The vendor ships updates, capabilities accumulate, and your installation gradually becomes a superset of what you actually use. The modules you don’t use still exist, still run, still represent potential exposure. The configuration options you don’t touch still need to be understood by whoever is responsible for keeping the system secure.</p>\n<p>NanoClaw inverts this. The base system is deliberately minimal. When you need a new capability - Gmail integration, a new communication channel, scheduled briefings - you add a skill: a set of instructions that transforms your installation to include exactly that capability. Not a toggle in a settings panel. An actual change to the code, scoped precisely to what you need, that you can read and understand before it goes live.</p>\n<p>The practical implication for a business is significant. A deployment that starts with document review and later needs to add regulatory compliance queries, or cross-reference supplier contracts, or monitor a specific set of public filings - each of those additions is deliberate, auditable, and reversible. The system grows with the business’s actual needs rather than arriving pre-loaded with capabilities that may or may not be relevant. There’s no inherited feature set to worry about. There’s no “we’re not sure exactly what that module does” answer to give an auditor.</p>\n<h2 id=\"the-harness-matters-as-much-as-the-model\">The harness matters as much as the model</h2>\n<p>One line in NanoClaw’s README deserves to be quoted in full: “A bad harness makes even smart models seem dumb, a good harness gives them superpowers.”</p>\n<p>This is the insight that separates thoughtful AI deployment from plugging an API key into a generic interface and measuring the results. The underlying model - however capable - performs within the constraints of the infrastructure around it. How context is structured and maintained. How tasks are isolated. How results are routed back. How the system handles the edge cases that the demos never show. These are harness decisions, and they determine whether the output is reliable, consistent, and useful under real working conditions rather than controlled demonstrations.</p>\n<p>NanoClaw makes a deliberate choice: run directly on the Claude Agent SDK, the same infrastructure that powers Claude Code, rather than abstracting away from it. The harness is as close to the model as it can be. That’s a principled engineering decision, and it’s one we recognise.</p>\n<p>The AI projects that will endure in <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">regulated businesses</a> aren’t the ones that offered the most features at the point of sale. They’re the ones where someone can still explain, years later, exactly what is running, what it can see, and why the output should be trusted. NanoClaw builds toward that standard. So do we.</p>\n<hr>\n<p><em>JD Fortress AI deploys secure, on-premises AI for businesses across regulated sectors in the UK. Get in touch for a confidential discussion - no pitch, just practical talk.</em></p>",{headings:238,localImagePaths:248,remoteImagePaths:249,frontmatter:250,imagePaths:252},[239,242,245],{depth:26,slug:240,text:241},"security-by-isolation-not-by-policy","Security by isolation, not by policy",{depth:26,slug:243,text:244},"a-system-that-grows-where-you-need-it","A system that grows where you need it",{depth:26,slug:246,text:247},"the-harness-matters-as-much-as-the-model","The harness matters as much as the model",[],[],{title:229,date:251,excerpt:231,author:17},["Date","2026-03-10T00:00:00.000Z"],[],"what-is-openclaw",{id:253,data:255,body:259,filePath:260,digest:261,rendered:262},{title:256,date:257,excerpt:258,author:17},"OpenClaw Changed My Life — And It’s About to Change How We Build AI for Businesses",["Date","2026-02-17T00:00:00.000Z"],"We’ve tracked this project from its early ClaudeBot days through Moltbot and now OpenClaw. What started as a quirky personal assistant has become the most compelling proof yet that autonomous, local AI agents are ready for real work. Here’s why we’re paying close attention.","If you spend your days deploying AI into environments where data simply cannot leave the premises - regulated FinTech, [legal practices](/blog/why-law-firms-cant-afford-cloud-ai), healthcare - you quickly learn the limits of even the best cloud-hosted models. Great at Q&A, terrible at sustained, reliable action without constant supervision or privacy trade-offs.\n\nThen OpenClaw appeared.\n\nWe started watching when it was still called Clawdbot (briefly Moltbot after the trademark shuffle). A small open-source repo that wrapped strong models like Claude into something that lived in your existing chat apps and actually executed tasks. We cloned it early, ran tests on spare servers, renamed forks internally, and followed the chaotic name changes and explosive growth (over 150k GitHub stars in weeks, community skills pouring in).\n\nWhat surprised us wasn’t the novelty - it was how quickly it stopped feeling like a toy. This was an agent that remembered context over weeks, woke itself up on a schedule, monitored channels, triaged issues, drafted responses in brand voice, even patched small bugs or generated diffs - all without anyone prompting every step.\n\n## What OpenClaw Actually Delivers (and Why the Architecture Matters)\n\nAt its core, OpenClaw is a self-hosted AI agent gateway. Install on a Mac Mini, Linux box, or VPS; point it at Claude, GPT-family, or local/open models; connect via WhatsApp, Telegram, Slack, Signal, Teams - whichever your team already uses.\n\nThe real difference comes from:\n\n- **Transparent, durable memory** - conversations and long-term context live as plain Markdown files + lightweight vector search. Human-readable, git-trackable, no black-box embeddings you can’t audit.\n- **Real tool access** - shell commands, browser control, file I/O, calendar/email integration, cron-like scheduling. Community \"skills\" (Anthropic-style) extend it fast - someone already built ones for CIS benchmark lookups, basic log parsing, even diff generation.\n- **Proactive heartbeat** - configurable wake-ups let it check inboxes, scan tickets, or monitor alerts independently.\n- **Local-first by design** - inference and memory stay on your hardware/network. Nothing ships to a third-party API unless you explicitly allow a cloud model.\n\nWe’ve run similar self-hosted [RAG](/blog/what-is-rag) + tool-calling setups for years (see our demo at cis.jdfortress.com). OpenClaw pushes further: it turns passive knowledge retrieval into goal-directed, autonomous behaviour.\n\n## Why This Matters for Data-Sensitive Businesses\n\nWe’ve always focused on on-prem/private RAG because regulations (GDPR, DPA 2018, NHS DSPT, FCA rules) leave no room for [public cloud leakage](/blog/chatgpt-discovery-legal-risk) in production.\n\nOpenClaw addresses the agency gap: real autonomy without handing control to a vendor.\n\n- Data never leaves your perimeter (OpenClaw is pointed at a local LLM).\n- Memory is inspectable and versionable.\n- Horizontal scaling: multiple role-specific agents (support, dev, compliance).\n- Community velocity is wild - skills library growing daily.\n\nThe upside is transformative - AI shifts from occasional helper to always-on teammate.\n\nFor us, this is the pattern we’ve waited for: open, local, extensible, compounding fast. We’re already adapting OpenClaw concepts into client-facing stacks - imagine domain-specific RAGs that’s constantly being updated and integrated into a truly agentic platform that proactively aids clients in their BAU workflow.\n\nIf your organisation has valuable internal knowledge, needs 24/7 monitoring/automation, and refuses to expose data - agentic local systems like this are shifting from experiment to necessity.\n\nWe’ve seen enough to be convinced. The future has claws - and it runs on your hardware.\n\n* * *\n\n*JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion - no pitch, just practical talk.*","src/content/blog/what-is-openclaw.md","d1216300047a366a",{html:263,metadata:264},"<p>If you spend your days deploying AI into environments where data simply cannot leave the premises - regulated FinTech, <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">legal practices</a>, healthcare - you quickly learn the limits of even the best cloud-hosted models. Great at Q&#x26;A, terrible at sustained, reliable action without constant supervision or privacy trade-offs.</p>\n<p>Then OpenClaw appeared.</p>\n<p>We started watching when it was still called Clawdbot (briefly Moltbot after the trademark shuffle). A small open-source repo that wrapped strong models like Claude into something that lived in your existing chat apps and actually executed tasks. We cloned it early, ran tests on spare servers, renamed forks internally, and followed the chaotic name changes and explosive growth (over 150k GitHub stars in weeks, community skills pouring in).</p>\n<p>What surprised us wasn’t the novelty - it was how quickly it stopped feeling like a toy. This was an agent that remembered context over weeks, woke itself up on a schedule, monitored channels, triaged issues, drafted responses in brand voice, even patched small bugs or generated diffs - all without anyone prompting every step.</p>\n<h2 id=\"what-openclaw-actually-delivers-and-why-the-architecture-matters\">What OpenClaw Actually Delivers (and Why the Architecture Matters)</h2>\n<p>At its core, OpenClaw is a self-hosted AI agent gateway. Install on a Mac Mini, Linux box, or VPS; point it at Claude, GPT-family, or local/open models; connect via WhatsApp, Telegram, Slack, Signal, Teams - whichever your team already uses.</p>\n<p>The real difference comes from:</p>\n<ul>\n<li><strong>Transparent, durable memory</strong> - conversations and long-term context live as plain Markdown files + lightweight vector search. Human-readable, git-trackable, no black-box embeddings you can’t audit.</li>\n<li><strong>Real tool access</strong> - shell commands, browser control, file I/O, calendar/email integration, cron-like scheduling. Community “skills” (Anthropic-style) extend it fast - someone already built ones for CIS benchmark lookups, basic log parsing, even diff generation.</li>\n<li><strong>Proactive heartbeat</strong> - configurable wake-ups let it check inboxes, scan tickets, or monitor alerts independently.</li>\n<li><strong>Local-first by design</strong> - inference and memory stay on your hardware/network. Nothing ships to a third-party API unless you explicitly allow a cloud model.</li>\n</ul>\n<p>We’ve run similar self-hosted <a href=\"/blog/what-is-rag\">RAG</a> + tool-calling setups for years (see our demo at cis.jdfortress.com). OpenClaw pushes further: it turns passive knowledge retrieval into goal-directed, autonomous behaviour.</p>\n<h2 id=\"why-this-matters-for-data-sensitive-businesses\">Why This Matters for Data-Sensitive Businesses</h2>\n<p>We’ve always focused on on-prem/private RAG because regulations (GDPR, DPA 2018, NHS DSPT, FCA rules) leave no room for <a href=\"/blog/chatgpt-discovery-legal-risk\">public cloud leakage</a> in production.</p>\n<p>OpenClaw addresses the agency gap: real autonomy without handing control to a vendor.</p>\n<ul>\n<li>Data never leaves your perimeter (OpenClaw is pointed at a local LLM).</li>\n<li>Memory is inspectable and versionable.</li>\n<li>Horizontal scaling: multiple role-specific agents (support, dev, compliance).</li>\n<li>Community velocity is wild - skills library growing daily.</li>\n</ul>\n<p>The upside is transformative - AI shifts from occasional helper to always-on teammate.</p>\n<p>For us, this is the pattern we’ve waited for: open, local, extensible, compounding fast. We’re already adapting OpenClaw concepts into client-facing stacks - imagine domain-specific RAGs that’s constantly being updated and integrated into a truly agentic platform that proactively aids clients in their BAU workflow.</p>\n<p>If your organisation has valuable internal knowledge, needs 24/7 monitoring/automation, and refuses to expose data - agentic local systems like this are shifting from experiment to necessity.</p>\n<p>We’ve seen enough to be convinced. The future has claws - and it runs on your hardware.</p>\n<hr>\n<p><em>JD Fortress AI builds secure, on-premises RAG and agent solutions for UK businesses in regulated sectors. If you’re exploring always-on, private AI teammates, get in touch for a confidential discussion - no pitch, just practical talk.</em></p>",{headings:265,localImagePaths:272,remoteImagePaths:273,frontmatter:274,imagePaths:276},[266,269],{depth:26,slug:267,text:268},"what-openclaw-actually-delivers-and-why-the-architecture-matters","What OpenClaw Actually Delivers (and Why the Architecture Matters)",{depth:26,slug:270,text:271},"why-this-matters-for-data-sensitive-businesses","Why This Matters for Data-Sensitive Businesses",[],[],{title:256,date:275,excerpt:258,author:17},["Date","2026-02-17T00:00:00.000Z"],[],"what-is-rag",{id:277,data:279,body:283,filePath:284,digest:285,rendered:286},{title:280,date:281,excerpt:282,author:17},"What is RAG and Why Does It Matter for Your Business?",["Date","2026-02-10T00:00:00.000Z"],"Retrieval-Augmented Generation lets AI answer questions using your own documents. Here's what it means, how it works, and why it's the missing piece for businesses that can't share their data with public AI.","If you've been trying to get your head around AI for your business, you've probably heard the term RAG. It sounds technical - and it is - but the concept is surprisingly straightforward, and the implications for businesses with sensitive data are significant.\n\n## What is RAG?\n\nRAG stands for Retrieval-Augmented Generation. It's a technique that combines two things:\n\n1. **A large language model** (like the AI behind ChatGPT) - good at reasoning, summarising, and generating coherent text.\n2. **A search system over your own documents** - so the AI can look things up in your files before answering.\n\nThe result: an AI that can answer questions using your own internal knowledge base, not just its general training data.\n\n## A Concrete Example\n\nImagine you're a [law firm](/blog/why-law-firms-cant-afford-cloud-ai) with 20 years of precedents, matter files, and internal guidance documents. Normally, a junior solicitor would spend hours searching for the right case or clause. With a RAG system built over your document library, you'd simply ask: \"What's our standard approach to limitation clauses in manufacturing contracts?\" - and get a cited, accurate answer drawn from your own files.\n\nNo hallucination. No generic advice. Your knowledge, made instantly searchable.\n\n## Why RAG Changes Everything for Data-Sensitive Businesses\n\nMost AI tools work by sending your questions - and sometimes your documents - to a remote server, where a model processes them and sends back an answer. For businesses with [confidential data](/blog/chatgpt-discovery-legal-risk), that's a problem.\n\nRAG can be deployed entirely on-premises. The language model runs inside your infrastructure. Your documents stay on your servers. The retrieval happens locally. Nothing leaves your network.\n\nThis is how JD Fortress AI builds RAG pipelines for our clients - the same architecture that powers FortiCIS, our CIS Benchmark assistant at cis.jdfortress.com.\n\n## What RAG Is Not\n\nRAG is not the same as giving an AI access to the internet. It's not a chatbot that guesses. And it's not a search engine that just returns links. It's a reasoning system that [retrieves relevant passages from a specific, controlled document set](/blog/context-is-not-free) — and uses them as the basis for a thoughtful, accurate response.\n\nThe \"retrieval\" step means the AI has evidence. The \"generation\" step means it can explain, summarise, and synthesise - not just list results.\n\n## Is RAG Right for Your Business?\n\nRAG is valuable for any organisation that:\n\n- Has accumulated significant internal knowledge (documents, policies, precedents, manuals)\n- Wants AI-powered search and Q&A over that knowledge\n- Can't risk sending documents to public cloud AI services\n- Needs cited, verifiable answers rather than AI guesswork\n\nIf that sounds like your business, we should talk.\n\n* * *\n\n*JD Fortress AI builds custom RAG pipelines for UK businesses. [Get in touch](/contact) for a confidential conversation.*","src/content/blog/what-is-rag.md","5019008a51cee98a",{html:287,metadata:288},"<p>If you’ve been trying to get your head around AI for your business, you’ve probably heard the term RAG. It sounds technical - and it is - but the concept is surprisingly straightforward, and the implications for businesses with sensitive data are significant.</p>\n<h2 id=\"what-is-rag\">What is RAG?</h2>\n<p>RAG stands for Retrieval-Augmented Generation. It’s a technique that combines two things:</p>\n<ol>\n<li><strong>A large language model</strong> (like the AI behind ChatGPT) - good at reasoning, summarising, and generating coherent text.</li>\n<li><strong>A search system over your own documents</strong> - so the AI can look things up in your files before answering.</li>\n</ol>\n<p>The result: an AI that can answer questions using your own internal knowledge base, not just its general training data.</p>\n<h2 id=\"a-concrete-example\">A Concrete Example</h2>\n<p>Imagine you’re a <a href=\"/blog/why-law-firms-cant-afford-cloud-ai\">law firm</a> with 20 years of precedents, matter files, and internal guidance documents. Normally, a junior solicitor would spend hours searching for the right case or clause. With a RAG system built over your document library, you’d simply ask: “What’s our standard approach to limitation clauses in manufacturing contracts?” - and get a cited, accurate answer drawn from your own files.</p>\n<p>No hallucination. No generic advice. Your knowledge, made instantly searchable.</p>\n<h2 id=\"why-rag-changes-everything-for-data-sensitive-businesses\">Why RAG Changes Everything for Data-Sensitive Businesses</h2>\n<p>Most AI tools work by sending your questions - and sometimes your documents - to a remote server, where a model processes them and sends back an answer. For businesses with <a href=\"/blog/chatgpt-discovery-legal-risk\">confidential data</a>, that’s a problem.</p>\n<p>RAG can be deployed entirely on-premises. The language model runs inside your infrastructure. Your documents stay on your servers. The retrieval happens locally. Nothing leaves your network.</p>\n<p>This is how JD Fortress AI builds RAG pipelines for our clients - the same architecture that powers FortiCIS, our CIS Benchmark assistant at cis.jdfortress.com.</p>\n<h2 id=\"what-rag-is-not\">What RAG Is Not</h2>\n<p>RAG is not the same as giving an AI access to the internet. It’s not a chatbot that guesses. And it’s not a search engine that just returns links. It’s a reasoning system that <a href=\"/blog/context-is-not-free\">retrieves relevant passages from a specific, controlled document set</a> — and uses them as the basis for a thoughtful, accurate response.</p>\n<p>The “retrieval” step means the AI has evidence. The “generation” step means it can explain, summarise, and synthesise - not just list results.</p>\n<h2 id=\"is-rag-right-for-your-business\">Is RAG Right for Your Business?</h2>\n<p>RAG is valuable for any organisation that:</p>\n<ul>\n<li>Has accumulated significant internal knowledge (documents, policies, precedents, manuals)</li>\n<li>Wants AI-powered search and Q&#x26;A over that knowledge</li>\n<li>Can’t risk sending documents to public cloud AI services</li>\n<li>Needs cited, verifiable answers rather than AI guesswork</li>\n</ul>\n<p>If that sounds like your business, we should talk.</p>\n<hr>\n<p><em>JD Fortress AI builds custom RAG pipelines for UK businesses. <a href=\"/contact\">Get in touch</a> for a confidential conversation.</em></p>",{headings:289,localImagePaths:304,remoteImagePaths:305,frontmatter:306,imagePaths:308},[290,292,295,298,301],{depth:26,slug:277,text:291},"What is RAG?",{depth:26,slug:293,text:294},"a-concrete-example","A Concrete Example",{depth:26,slug:296,text:297},"why-rag-changes-everything-for-data-sensitive-businesses","Why RAG Changes Everything for Data-Sensitive Businesses",{depth:26,slug:299,text:300},"what-rag-is-not","What RAG Is Not",{depth:26,slug:302,text:303},"is-rag-right-for-your-business","Is RAG Right for Your Business?",[],[],{title:280,date:307,excerpt:282,author:17},["Date","2026-02-10T00:00:00.000Z"],[],"why-law-firms-cant-afford-cloud-ai",{id:309,data:311,body:315,filePath:316,digest:317,rendered:318},{title:312,date:313,excerpt:314,author:17},"Why High Street Law Firms Can’t Afford Cloud AI",["Date","2026-02-03T00:00:00.000Z"],"Cloud AI tools promise efficiency. But for law firms, every document you upload could be a professional conduct breach. Here’s what partners need to know.","Over the past few months we have sat down with several people working right at the coalface of legal AI adoption in the UK: partners at mid-sized High Street firms, [in-house legal teams](/blog/gen-ai-for-in-house-lawyers) at corporates, and sales reps who spend their days pitching tools to cautious practices.\n\nAlex runs compliance at a 15-partner firm in Hertfordshire—mostly property, family, and small commercial work. He told me straight: “We trialled one of the big-name cloud legal AIs last year. The speed was impressive, but as soon as someone asked ‘Where exactly is our clients’ data going?’, the conversation stopped. No one could give a clean answer that satisfied the risk register.”\n\nJack, a partner at a long-established London practice, put it more bluntly over coffee: “I’m not anti-AI. I just can’t be the one who explains to the SRA why we sent Mrs Johnson’s divorce file to a server in California because it saved us three hours on disclosure review.”\n\nSamantha works in legal tech sales and covers the full spectrum — from Magic Circle down to two-partner outfits. She sees the same pattern repeatedly: “The enthusiasm is there until the compliance partner or the MLRO gets involved. Then it’s all about privilege, confidentiality, and whether the tool’s terms of service create an unauthorised disclosure. Most cloud providers still can’t square that circle for regulated work.”\n\n## The SRA Principle 6 Reality\n\nUnder SRA Principle 6 (and paragraph 6.3 of the Code of Conduct for Solicitors), confidentiality to clients is absolute — not “reasonable efforts,” not “best endeavours.” Uploading client documents to any third-party cloud service means [the data leaves your controlled environment](/blog/notebooklm-lawyer-lockout-warning). Even with a data processing addendum in place, the SRA’s position on whether that amounts to impermissible disclosure remains unsettled in 2026. Their latest compliance tips (updated February 2026) remind firms that technology use must still satisfy the full suite of principles, including secure handling of client information. Guidance is evolving, but no one wants to be the test case.\n\n## The CLOUD Act Isn’t Going Away\n\nMost of the popular legal AI platforms — whether from US-headquartered providers like OpenAI, Microsoft, Thomson Reuters, or others — fall under US jurisdiction. That brings the [CLOUD Act](/blog/chatgpt-discovery-legal-risk) into play: US authorities can compel those companies to hand over data, even if it’s stored in European data centres. Recent commentary from law firms and data protection specialists underlines that the US–UK Data Access Agreement hasn’t eliminated the underlying exposure; it’s simply streamlined one channel for law enforcement requests.\n\nFor a High Street firm handling personal injury, conveyancing, or probate, that’s not an abstract risk — it’s a potential breach of client trust that no professional indemnity insurer wants to defend.\n\n## What the Careful Firms Are Actually Doing\n\nThe practices that are getting real value from AI aren’t rejecting it — they’re insisting on architectures that keep the risk profile acceptable.\n\n[On-premises (or private VPC/air-gapped) deployment](/blog/what-is-rag) is the route many are taking. The model and the inference happen inside the firm’s own perimeter: no documents transit the public internet, no third-party processor touches the data, no CLOUD Act reach. The productivity lift — faster research, document summarisation, drafting first cuts — remains the same, but the compliance headache disappears.\n\n## The Bottom Line for High Street Practices\n\nOn-premises or private AI isn’t futuristic or only for the City. Modern solutions make it realistic for firms of 2–50 solicitors without massive internal IT overhead.\nThe real question isn’t “Can we afford to run our own AI stack?” It’s “Can we afford the regulatory, reputational, and client-trust cost of not doing so?”\n\nIf you’re a partner or compliance lead weighing the same trade-offs Alex, Jack, and their peers are, we’re happy to talk through what a realistic, low-friction on-premises option looks like for your firm — no hard sell, just a confidential conversation.\n\n* * *\n\n*JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.*","src/content/blog/why-law-firms-cant-afford-cloud-ai.md","96359f8e048f6ec9",{html:319,metadata:320},"<p>Over the past few months we have sat down with several people working right at the coalface of legal AI adoption in the UK: partners at mid-sized High Street firms, <a href=\"/blog/gen-ai-for-in-house-lawyers\">in-house legal teams</a> at corporates, and sales reps who spend their days pitching tools to cautious practices.</p>\n<p>Alex runs compliance at a 15-partner firm in Hertfordshire—mostly property, family, and small commercial work. He told me straight: “We trialled one of the big-name cloud legal AIs last year. The speed was impressive, but as soon as someone asked ‘Where exactly is our clients’ data going?’, the conversation stopped. No one could give a clean answer that satisfied the risk register.”</p>\n<p>Jack, a partner at a long-established London practice, put it more bluntly over coffee: “I’m not anti-AI. I just can’t be the one who explains to the SRA why we sent Mrs Johnson’s divorce file to a server in California because it saved us three hours on disclosure review.”</p>\n<p>Samantha works in legal tech sales and covers the full spectrum — from Magic Circle down to two-partner outfits. She sees the same pattern repeatedly: “The enthusiasm is there until the compliance partner or the MLRO gets involved. Then it’s all about privilege, confidentiality, and whether the tool’s terms of service create an unauthorised disclosure. Most cloud providers still can’t square that circle for regulated work.”</p>\n<h2 id=\"the-sra-principle-6-reality\">The SRA Principle 6 Reality</h2>\n<p>Under SRA Principle 6 (and paragraph 6.3 of the Code of Conduct for Solicitors), confidentiality to clients is absolute — not “reasonable efforts,” not “best endeavours.” Uploading client documents to any third-party cloud service means <a href=\"/blog/notebooklm-lawyer-lockout-warning\">the data leaves your controlled environment</a>. Even with a data processing addendum in place, the SRA’s position on whether that amounts to impermissible disclosure remains unsettled in 2026. Their latest compliance tips (updated February 2026) remind firms that technology use must still satisfy the full suite of principles, including secure handling of client information. Guidance is evolving, but no one wants to be the test case.</p>\n<h2 id=\"the-cloud-act-isnt-going-away\">The CLOUD Act Isn’t Going Away</h2>\n<p>Most of the popular legal AI platforms — whether from US-headquartered providers like OpenAI, Microsoft, Thomson Reuters, or others — fall under US jurisdiction. That brings the <a href=\"/blog/chatgpt-discovery-legal-risk\">CLOUD Act</a> into play: US authorities can compel those companies to hand over data, even if it’s stored in European data centres. Recent commentary from law firms and data protection specialists underlines that the US–UK Data Access Agreement hasn’t eliminated the underlying exposure; it’s simply streamlined one channel for law enforcement requests.</p>\n<p>For a High Street firm handling personal injury, conveyancing, or probate, that’s not an abstract risk — it’s a potential breach of client trust that no professional indemnity insurer wants to defend.</p>\n<h2 id=\"what-the-careful-firms-are-actually-doing\">What the Careful Firms Are Actually Doing</h2>\n<p>The practices that are getting real value from AI aren’t rejecting it — they’re insisting on architectures that keep the risk profile acceptable.</p>\n<p><a href=\"/blog/what-is-rag\">On-premises (or private VPC/air-gapped) deployment</a> is the route many are taking. The model and the inference happen inside the firm’s own perimeter: no documents transit the public internet, no third-party processor touches the data, no CLOUD Act reach. The productivity lift — faster research, document summarisation, drafting first cuts — remains the same, but the compliance headache disappears.</p>\n<h2 id=\"the-bottom-line-for-high-street-practices\">The Bottom Line for High Street Practices</h2>\n<p>On-premises or private AI isn’t futuristic or only for the City. Modern solutions make it realistic for firms of 2–50 solicitors without massive internal IT overhead.\nThe real question isn’t “Can we afford to run our own AI stack?” It’s “Can we afford the regulatory, reputational, and client-trust cost of not doing so?”</p>\n<p>If you’re a partner or compliance lead weighing the same trade-offs Alex, Jack, and their peers are, we’re happy to talk through what a realistic, low-friction on-premises option looks like for your firm — no hard sell, just a confidential conversation.</p>\n<hr>\n<p><em>JD Fortress AI deploys secure, on-premises AI for law firms across the UK. Get in touch for a no-obligation discussion.</em></p>",{headings:321,localImagePaths:334,remoteImagePaths:335,frontmatter:336,imagePaths:338},[322,325,328,331],{depth:26,slug:323,text:324},"the-sra-principle-6-reality","The SRA Principle 6 Reality",{depth:26,slug:326,text:327},"the-cloud-act-isnt-going-away","The CLOUD Act Isn’t Going Away",{depth:26,slug:329,text:330},"what-the-careful-firms-are-actually-doing","What the Careful Firms Are Actually Doing",{depth:26,slug:332,text:333},"the-bottom-line-for-high-street-practices","The Bottom Line for High Street Practices",[],[],{title:312,date:337,excerpt:314,author:17},["Date","2026-02-03T00:00:00.000Z"],[]];

export { _astro_dataLayerContent as default };
